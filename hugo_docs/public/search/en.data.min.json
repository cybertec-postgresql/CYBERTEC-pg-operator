[{"id":0,"href":"/CYBERTEC-pg-operator/","title":"CPO (CYBERTEC-PG-Operator)","parent":"","content":"Current Release: 0.8.2 (12.12.2024) Release Notes\nCPO (CYBERTEC PG Operator) allows you to create and run PostgreSQL clusters on Kubernetes.\nThe operator reduces your efforts and simplifies the administration of your PostgreSQL clusters so that you can concentrate on other things.\nThe following features characterise our operator:\nDeclarative mode of operation Takes over all the necessary steps for setting up and managing the PG cluster. Integrated backup solution, automatic backups and very easy restore (snapshot \u0026amp; PITR) Rolling update procedure for adjustments to the pods and minor updates Major upgrade with minimum interruption time Reduction of downtime thanks to redundancy, pod anti-affinity, auto-failover and self-healing CPO is tested on the following platforms:\nKubernetes: 1.21 - 1.28 Openshift: 4.8 - 4.13 Rancher AWS EKS Azure AKS Google GKE Furthermore, CPO is basically executable on any CSCF-certified Kubernetes platform.\n","description":"Current Release: 0.8.2 (12.12.2024) Release Notes\nCPO (CYBERTEC PG Operator) allows you to create and run PostgreSQL clusters on Kubernetes.\nThe operator reduces your efforts and simplifies the administration of your PostgreSQL clusters so that you can concentrate on other things.\nThe following features characterise our operator:\nDeclarative mode of operation Takes over all the necessary steps for setting up and managing the PG cluster. Integrated backup solution, automatic backups and very easy restore (snapshot \u0026amp; PITR) Rolling update procedure for adjustments to the pods and minor updates Major upgrade with minimum interruption time Reduction of downtime thanks to redundancy, pod anti-affinity, auto-failover and self-healing CPO is tested on the following platforms:\n"},{"id":1,"href":"/CYBERTEC-pg-operator/backup/introduction/","title":"Introduction","parent":"Backup","content":"Backups are essential for databases. From broken storage to deployments gone wrong, backups often save the day. Starting with pg_dump, which was released in the late 1990s, to the archiving of WAL files (PostgreSQL 8.0 / 2005) and pg_basebackup (PostgreSQL 9.0 / 2010), PostgreSQL already offers built-in options for backups and restores based on logical and physical backups.\nBackups with pgBackRest CPO relies on pgBackRest as its backup solution, a tried-and-tested tool with extensive backup and restore options. The backup is based on two elements:\nSnapshots in the form of physical backups WAL archive: Continuous archiving of the WAL files Backups Backups represent a snapshot of the database in the form of pyhsical files. This contains all relevant information that PostgreSQL holds in its data folder. With pgBackRest it is possible to create different types of Backups:\nfull Snapshot: This captures and saves all files at the time of the backup Differential backup: Only captures all files that have been changed since the last full Backup Incremental backup: Only records the files that have been changed since the last backup (of any kind). When restoring using differential or incremental Backup, it is necessary to also use the previous Backup that provide the basis for the selected Backup.\nHINT: The choice of Backup types depends on factors such as the size of the database, the time available for backups and the restore.\nWAL-Archive The WAL (Write-Ahead-Log) refers to log files which record all changes to the database data before they are written to the actual files. The basic idea here is to guarantee the consistency and recoverability of the comitted data even in the event of failures.\nPostgreSQL normally cleans up or recycles the WAL files that are no longer required. By using WAL archiving, the WAL files are saved to a different location before this process so that they can be used for various activities in the future. These activities include\nProviding the WAL files for replicas to keep them up to date Restoring instances that have lost parts of the WAL files in the event of a failure and cannot return to a consistent state without them without losing data Point-In-Time-Recovery (PITR): In contrast to Backups, which map a fixed point in time, WAL files make it possible to jump dynamically to a desired point in time and restore the database to the closest available consistent data point HINT: WAL archiving is an indispensable tool for data availability, recoverability and the continuous availability of PostgreSQL.\nBackup your Cluster With pgBackRest, backups can be stored on different types of storage:\nBlock storage (PVC) S3 / S3-compatible storage Azure blob storage GCS How a Backup works The operator creates a cronjob object on Kubernetes based on the defined times for automatic backups. This means that the Kubernetes core (CronJob Controller) will take care of processing the automatic backups and create a job and thus a pod at the appropriate time. The pod will send the backup command to the primary or, if block storage is used, to the repo host and monitor it. As soon as the backup is successfully completed, the pod stops with Completed and thus completes the job.\nkubectl get cronjobs --------------------------------------------------------------------------------------- NAME | SCHEDULE | SUSPEND | ACTIVE | LAST SCHEDULE | AGE pgbackrest-cluster-repo1-full | 30 2 * * * | False | 0 | 4h46m | 14h pgbackrest-cluster-repo1-incr | */30 * * * * | False | 1 | 81s | 106m kubectl get jobs ----------------------------------------------------------------------- NAME | COMPLETIONS | DURATION | AGE pgbackrest-cluster-repo1-full-28597110 | 1/1 | 52s | 140m pgbackrest-cluster-repo1-incr-28597365 | 1/1 | 2m37s | 32m pgbackrest-cluster-repo1-incr-28597380 | 1/1 | 2m38s | 17m pgbackrest-cluster-repo1-incr-28597395 | 0/1 | 2m3s | 2m3s If there are problems such as a timeout, the pod will stop with exit code 1 and thus indicate an error. In this case, a new pod will be created which will attempt to complete the backup. The maximum number of attempts is 6, so if the backup fails six times, the job is deemed to have failed and will not be attempted again until the next cronjob execution. The job pod log provides information about the problems.\nkubectl get pods ----------------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cluster-0 | 2/2 | Running | 2 | 14h cluster-pgbackrest-repo-host-0 | 1/1 | Running | 0 | 107m pgbackrest-cluster-repo1-full-28597110-x8zpw | 0/1 | Completed | 0 | 143m pgbackrest-cluster-repo1-incr-28597365-7bb5l | 0/1 | Completed | 0 | 34m pgbackrest-cluster-repo1-incr-28597380-j76rr | 0/1 | Completed | 0 | 19m pgbackrest-cluster-repo1-incr-28597395-rh86t | 0/1 | Completed | 0 | 4m27s postgres-operator-66bbff5c54-5sjmk | 1/1 | Running | 0 | 47m ","description":"Backups are essential for databases. From broken storage to deployments gone wrong, backups often save the day. Starting with pg_dump, which was released in the late 1990s, to the archiving of WAL files (PostgreSQL 8.0 / 2005) and pg_basebackup (PostgreSQL 9.0 / 2010), PostgreSQL already offers built-in options for backups and restores based on logical and physical backups.\nBackups with pgBackRest CPO relies on pgBackRest as its backup solution, a tried-and-tested tool with extensive backup and restore options. The backup is based on two elements:\n"},{"id":2,"href":"/CYBERTEC-pg-operator/customize_cluster/sidecars/","title":"Sidecars","parent":"Customize Cluster","content":"Starting with the Single-Node-Cluster from the previous section, we want to modify the Instance a bit to see.\nCPU and Memory spec: resources: limits: cpu: 1000m memory: 500Mi requests: cpu: 500m memory: 500mi Based on the ressources-Definiton we\u0026rsquo;re able to modify the reserved Hardware (requests) and the limits, which allows use to consume more than the reserved definitons if the k8s-worker has this hardware available. There are some Restrictions when modifiying the limits-section. Because of the behaviour of Databases we should never define a diff between requests.memory and limits.memory. A Database is after some time using all available Memory, for Cache and other things. Limits are optional and the worker node can force them back. forcing back memory will create big problems inside a database like creating corruption, forcing OutOfMemory-Killer and so on. CPU on the other side is a ressource we can use inside the limits definiton to allow our database using more cpu if needed and available.\nSidecars Sidecars are further Containers running on the same Pod as the Database. We can use them for serveral different Jobs. The Operator allows us to define them directly inside the Cluster-Manifest.\nspec: sidecars: - name: \u0026#34;telegraf-sidecar\u0026#34; image: \u0026#34;telegraf:latest\u0026#34; ports: - name: metrics containerPort: 8094 protocol: TCP resources: limits: cpu: 500m memory: 500Mi requests: cpu: 100m memory: 100Mi env: - name: \u0026#34;USEFUL_VAR\u0026#34; value: \u0026#34;perhaps-true\u0026#34; This Example will add a second Container to our Pods. This will trigger a restart, which creates Downtime if you\u0026rsquo;re not running a HA-Cluster.\nInit-Containers We can exactly the same as for sidecars also for Init-Containers. The difference is, that a sidecar is running normally on a pod. An Init-Container will just run as first container when the pod is created and it will ends after his job is done. The \u0026ldquo;normal\u0026rdquo; Containers has to wait till all init-Containers finished their jobs and ended with a exit-status.\nspec: initContainers: - name: date image: busybox command: [ \u0026#34;/bin/date\u0026#34; ] TLS-Certificates One Startup the Containers will create a custom TLS-Certificate which allows creating tls-secured-connections to the Database. But this Certificates cannot verified, because the application has no information about the CA. Because of this the certificates are no protection against MITM-Attacks. You\u0026rsquo;re able to configure your own Certificates and CA to ensure, that you can use secured and verified connections between your application and your database.\nspec: tls: secretName: \u0026#34;\u0026#34; # should correspond to a Kubernetes Secret resource to load certificateFile: \u0026#34;tls.crt\u0026#34; privateKeyFile: \u0026#34;tls.key\u0026#34; caFile: \u0026#34;\u0026#34; # optionally configure Postgres with a CA certificate caSecretName: \u0026#34;\u0026#34; # optionally the ca.crt can come from this secret instead. You need to store the needed values from tls.crt, tls.key and ca.crt in a secret and define the secrtetname inside the tls-object. if you want you can create a separate sercet just for the ca and use this secret for every cluster inside the Namespace. To get Information about creating Certificates and the secrets check the Tutorial in the additonal-Section or click here\nNode-Affinity Node-Affinity will ensure that the Cluster-pods only deployed on Kubernetes-Nodes which has the defined Labelkey and -Value\nspec: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: cpo operator: In values: - enabled This allowes you to use specific database-nodes in a mixed cluster for example. In the Example above the Cluster-Pods are just deployed on Nodes with the Key: cpo and the value: enabled So you\u0026rsquo;re able to seperate your Workload.\nPostgreSQL-Configuration Every Cluster will start with the default PostgreSQL-Configuration. Every Parameter can be overriden based in definitions inside the Cluster-Manifest. Therefore we just need a add the section parameters to the postgresql-Object\nspec: postgresql: version: 16 parameters: max_connections: \u0026#34;53\u0026#34; log_statement: \u0026#34;all\u0026#34; track_io_timing: \u0026#34;true\u0026#34; These Definitions will change the PostgreSQL-Configuration. Based on the needs of Parameter changes the Pods may needs a restart, which creates a Downtime if its not a HA-Cluster. You can check Parameters and allowed Values on this Sources to ensure a correct Value.\nPostgreSQL Documentation PostgreSQL.org PostgreSQLco.nf ","description":"Starting with the Single-Node-Cluster from the previous section, we want to modify the Instance a bit to see.\nCPU and Memory spec: resources: limits: cpu: 1000m memory: 500Mi requests: cpu: 500m memory: 500mi Based on the ressources-Definiton we\u0026rsquo;re able to modify the reserved Hardware (requests) and the limits, which allows use to consume more than the reserved definitons if the k8s-worker has this hardware available. There are some Restrictions when modifiying the limits-section. Because of the behaviour of Databases we should never define a diff between requests.memory and limits.memory. A Database is after some time using all available Memory, for Cache and other things. Limits are optional and the worker node can force them back. forcing back memory will create big problems inside a database like creating corruption, forcing OutOfMemory-Killer and so on. CPU on the other side is a ressource we can use inside the limits definiton to allow our database using more cpu if needed and available.\n"},{"id":3,"href":"/CYBERTEC-pg-operator/backup/pvc/","title":"via Blockstorage (pvc)","parent":"Backup","content":" Backups on PVC (PersistentVolumeClaim) When using block storage, the operator creates an additional pod that acts as a repo host. Based on a TLS connection, the repo host obtains the data for the Backup from the current primary of the cluster, which is compressed before being sent. WAL archives are pushed from the primary pod to the repo host.\napiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster namespace: cpo spec: backup: pgbackrest: image: \u0026#39;docker.io/cybertecpostgresql/cybertec-pg-container:pgbackrest-16.4-1\u0026#39; repos: - name: repo1 schedule: full: 30 2 * * * storage: pvc volume: size: 15Gi storageClass: default global: repo1-retention-full: \u0026#39;7\u0026#39; repo1-retention-full-type: count This example creates backups based on a repo host with a daily full Backup at 2:30 am. In addition, pgBackRest is instructed to keep a maximum of 7 full Backups. The oldest one is always removed when a new Backup is created. You can increase the pvc-size all time if needed. Therefore you just need to update the size value to a higher amount of Gi. Please be aware that shrinking the volume is not possible.\nIn addition, further configurations for pgBackRest can be defined in the global object. Information on possible configurations can be found in the pgBackRest documentation ","description":" Backups on PVC (PersistentVolumeClaim) When using block storage, the operator creates an additional pod that acts as a repo host. Based on a TLS connection, the repo host obtains the data for the Backup from the current primary of the cluster, which is compressed before being sent. WAL archives are pushed from the primary pod to the repo host.\n"},{"id":4,"href":"/CYBERTEC-pg-operator/architecture/architecture/","title":"Architecture","parent":"Architecture","content":"This chapter covers all important aspects relating to the architecture of CPO and the associated components. In addition to the underlying Kubertnetes, the various components and their interaction for the operation of a PostgreSQL cluster are analysed.\nBrief overview of the components Network-Traffic PG-Cluster-intern Traffic With internal PG cluster-internal traffic, we are talking about all traffic that is necessary for the operation of the cluster itself. This includes\nCommunication for the sync of the replicas: pg_basebackup \u0026amp; streaming replication Communication with pgBackRest (if configured) Backups WAL archiving replica-create for new replicas The figure below shows the internal traffic flows with pgBackRest based on block storage (left) or cloud storage (right)\nExternal Traffic External traffic, i.e. the connection to the database for the user or the application, takes place via defined Kubernetes services. A distinction must be made here between read/write and read only traffic.\nread/write read-only ","description":"This chapter covers all important aspects relating to the architecture of CPO and the associated components. In addition to the underlying Kubertnetes, the various components and their interaction for the operation of a PostgreSQL cluster are analysed.\nBrief overview of the components Network-Traffic PG-Cluster-intern Traffic With internal PG cluster-internal traffic, we are talking about all traffic that is necessary for the operation of the cluster itself. This includes\n"},{"id":5,"href":"/CYBERTEC-pg-operator/backup/aws/","title":"via S3","parent":"Backup","content":"This chapter describes the use of pgBackRest in combination with with AWS S3 or S3-compatible storage such as MinIO, Cloudian HyperStore or SwiftStack. It is not absolutely necessary to operate a Kubernetes on the AWS Cloud Platform. However, as with any cloud storage, the efficiency and therefore the duration of a backup depends on the connection.\nThis Chapter will use AWS S3 for the example, the usage of different s3-compatible Storage is similiar.\nPrecondition: a S3-bucket and a priviledged role with credentials is needed for this chapter. Create a s3-bucket on the AWS console Create a priviledged service-role Modifying the Cluster As soon as all requirements are met:\nA S3 bucket Access-Token and Secret-Access-Key for the service role with the required authorisations for the bucket the cluster can be modified. Firstly, a secret containing the Credentials is created and the cluster manifest is adapted accordingly.\nThe first step is to create the required secret. This is most easily done storing the needed data in a file called s3.conf and using a kubectl command.\n# Create a file with name s3.conf and add the following infos. Please replace the placeholder by the credentials [global] repo1-s3-key=YOUR_S3_ACCESS_KEY repo1-s3-key-secret=YOUR_S3_KEY_SECRET repo1-cipher-pass=YOUR_ENCRYPTION_PASSPHRASE # Create the secret with the credentials kubectl create secret generic cluster-1-s3-credentials --from-file=s3.conf=s3.conf In the next step, the secret name ais stored in the secret in the cluster manifest. In addition, global settings, such as the retention time of the backups in the global object, are defined, the image for pgBackRest is specified and the necessary information for the repository is added. This includes both the desired storage path in the bucket and the times for automatic backups based on the cron syntax.\napiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster namespace: cpo spec: backup: pgbackrest: image: \u0026#39;docker.io/cybertecpostgresql/cybertec-pg-container:pgbackrest-16.4-1\u0026#39; repos: - endpoint: \u0026#39;https://s3-zurich.cyberlink.cloud:443\u0026#39; name: repo1 region: zurich resource: cpo-cluster-bucket schedule: full: 30 2 * * * incr: \u0026#39;*/30 * * * *\u0026#39; storage: s3 configuration: secret: cluster-1-s3-credential global: repo1-path: /cluster/repo1/ repo1-retention-full: \u0026#39;7\u0026#39; repo1-retention-full-type: count This example creates a backup in the defined S3 bucket. In addition to the above configurations, a secret is also required which contains the access data for the S3 storage. The name of the secret must be stored in the spec.backup.pgbackrest.configuration.secret object and the secret must be located in the same namespace as the cluster. Information required to address the S3 bucket:\nEndpoint: S3 api endpoint Region: Region of the bucket resource: Name of the bucket An example with a sercret generator is also available in the tutorials. Enter your access data in the s3.conf file and transfer the tutorial to your Kubernetes with kubectl apply -k cluster-tutorials/pgbackrest_with_s3/.\n","description":"This chapter describes the use of pgBackRest in combination with with AWS S3 or S3-compatible storage such as MinIO, Cloudian HyperStore or SwiftStack. It is not absolutely necessary to operate a Kubernetes on the AWS Cloud Platform. However, as with any cloud storage, the efficiency and therefore the duration of a backup depends on the connection.\nThis Chapter will use AWS S3 for the example, the usage of different s3-compatible Storage is similiar.\n"},{"id":6,"href":"/CYBERTEC-pg-operator/architecture/compontens/","title":"Software-Components","parent":"Architecture","content":"Various software components are used to operate CPO. This chapter lists the most important components and their respective purposes.\nBasically, the CPO project focusses on the main tasks of each individual component. This means that each component does what it does best and only that. In addition to reliable operation, this should also ensure efficient development and project management that utilises existing approaches rather than fighting against them.\n1. CYBERTEC-pg-operator The CYBERTEC-pg-operator is a Kubernetes operator that automates the operation and management of PostgreSQL databases on Kubernetes clusters. It facilitates the provisioning, scaling, backup and recovery of PostgreSQL clusters and integrates tools such as Patroni and pgBackRest for high availability and backup management.\nThe main focus of the operator is the creation of the necessary templates and objects for Kubernetes, the regular check whether the declarative description of the cluster is still up to date and for the implementation of various tasks in the cluster, which were commissioned by the user.\n2. Kubernetes Kubernetes is an open source platform for automating the deployment, scaling and management of containerised applications. It enables the management of container clusters in different environments and offers functions such as automatic load balancing, self-healing and rollouts. Kubernetes ensures that applications are always available and scalable and provides a framework for managing infrastructure in a cloud-native environment.\nThe focus of Kubernetes in the context of CPO is the use of the operator\u0026rsquo;s templates to create the required objects. For example, the statefulset controller creates the desired pods based on the template. Kubernetes or the respective controllers monitor the generated objects independently and react if they are missing or do not correspond to the template. This means, for example, that pods that have been removed are automatically regenerated even if the operator is not currently running. This avoids the operator as a single point of failure.\n3. Patroni Patroni is an open source tool for managing PostgreSQL high availability clusters. It uses a distributed consensus mechanism, often based on Etcd, Consul or Zookeeper, to manage the role of the PostgreSQL primary node and perform automatic failovers. Patroni ensures that only one primary database server is active at a time, enabling consistency and availability of PostgreSQL databases in a cluster.\nThe focus of Patroni is to build, configure and monitor the PostgreSQL cluster based on the configuration created by the operator. Patroni therefore takes over all tasks such as leader selection, cluster monitoring, auto-failover and much more independently. Patroni is included in every PostgreSQL container and therefore pod and focussed on the individual cluster. This means that cluster management is guaranteed even without a currently running operator and therefore runs independently of the operator. This avoids the operator as a single point of failure.\n4. PostgreSQL PostgreSQL is a powerful, open source object-relational database management system (ORDBMS). It is known for its reliability, robustness and compliance with SQL standards. PostgreSQL supports advanced data types, functions and offers extensive customisation options. It is suitable for applications of any size and offers strong support for ACID transactions and Multi-Version Concurrency Control (MVCC).\nThe main role of PostgreSQL in the context of CPO is quite clear. Controlled by Patroni, PostgreSQL takes care of its task as a DBMS.\n5. pgBackRest pgBackRest is a reliable backup and restore tool for PostgreSQL databases. It offers features such as incremental backups, parallel backup and restore, compression and encryption. pgBackRest is designed for use in large databases and offers both local and remote backup options. It integrates well into Kubernetes environments and enables automated and efficient backup strategies.\npgBackRest is configured based on the cluster manifest and therefore via the operator. Automatic backups, on the other hand, are based on Kubernetes cron jobs and are therefore independent of the operator, apart from the template generation by the operator.\n6. pgBouncer PgBouncer is a lightweight connection pooler for PostgreSQL. It reduces the load on the database server by consolidating and efficiently managing incoming client connections. PgBouncer improves the performance and scalability of PostgreSQL-based applications by reducing the number of active connections while enabling fast switching times between different connections.\n","description":"Various software components are used to operate CPO. This chapter lists the most important components and their respective purposes.\nBasically, the CPO project focusses on the main tasks of each individual component. This means that each component does what it does best and only that. In addition to reliable operation, this should also ensure efficient development and project management that utilises existing approaches rather than fighting against them.\n1. CYBERTEC-pg-operator The CYBERTEC-pg-operator is a Kubernetes operator that automates the operation and management of PostgreSQL databases on Kubernetes clusters. It facilitates the provisioning, scaling, backup and recovery of PostgreSQL clusters and integrates tools such as Patroni and pgBackRest for high availability and backup management.\n"},{"id":7,"href":"/CYBERTEC-pg-operator/customize_cluster/additional-volumes/","title":"Additional Volumes","parent":"Customize Cluster","content":" additionalVolumes: - name: empty mountPath: /opt/empty targetContainers: - all volumeSource: emptyDir: {} # - name: data # mountPath: /home/postgres/pgdata/partitions # targetContainers: # - postgres # volumeSource: # PersistentVolumeClaim: # claimName: pvc-postgresql-data-partitions # readyOnly: false # - name: conf # mountPath: /etc/telegraf # subPath: telegraf.conf # targetContainers: # - telegraf-sidecar # volumeSource: # configMap: # name: my-config-map ","description":" additionalVolumes: - name: empty mountPath: /opt/empty targetContainers: - all volumeSource: emptyDir: {} # - name: data # mountPath: /home/postgres/pgdata/partitions # targetContainers: # - postgres # volumeSource: # PersistentVolumeClaim: # claimName: pvc-postgresql-data-partitions # readyOnly: false # - name: conf # mountPath: /etc/telegraf # subPath: telegraf.conf # targetContainers: # - telegraf-sidecar # volumeSource: # configMap: # name: my-config-map "},{"id":8,"href":"/CYBERTEC-pg-operator/backup/gcs/","title":"via GCS","parent":"Backup","content":"This chapter describes the use of pgBackRest in combination with Google Cloud Storage (gcs). It is not absolutely necessary to operate a Kubernetes on the Google Cloud Platform. However, as with any cloud storage, the efficiency and therefore the duration of a backup depends on the connection.\nPrecondition: a gcs-bucket and a priviledged role is needed for this chapter. Create a gcs-bucket on the google cloud console Create a priviledged service-role Modifying the Cluster As soon as all requirements are met:\nA GCS bucket A JSON token for the service role with the required authorisations for the bucket the cluster can be modified. Firstly, a secret containing the JSON token is created and the cluster manifest is adapted accordingly.\nThe first step is to create the required secret. This is most easily done using a kubectl command.\nkubectl create secret generic cluster-1-gcs-credentials --from-file=gcs.json=fluent.json In the next step, both the secret name and the file name of the JSON token are stored in the secret in the cluster manifest. In addition, global settings, such as the retention time of the backups in the global object, are defined, the image for pgBackRest is specified and the necessary information for the repository is added. This includes both the desired storage path in the bucket and the times for automatic backups based on the cron syntax.\napiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: backup: pgbackrest: configuration: secret: cluster-1-gcs-credentials global: repo1-path: /cluster-1/repo1/ repo1-retention-full: \u0026#39;7\u0026#39; repo1-retention-full-type: count image: docker.io/cybertecpostgresql/cybertec-pg-container:pgbackrest-16.4-1\u0026#39; repos: - name: repo1 resource: postgresql-backup-bucket key: gcs.json keyType: service schedule: full: 30 2 * * * storage: gcs ","description":"This chapter describes the use of pgBackRest in combination with Google Cloud Storage (gcs). It is not absolutely necessary to operate a Kubernetes on the Google Cloud Platform. However, as with any cloud storage, the efficiency and therefore the duration of a backup depends on the connection.\nPrecondition: a gcs-bucket and a priviledged role is needed for this chapter. Create a gcs-bucket on the google cloud console Create a priviledged service-role Modifying the Cluster As soon as all requirements are met:\n"},{"id":9,"href":"/CYBERTEC-pg-operator/architecture/rolling_update/","title":"Rolling-Updates","parent":"Architecture","content":"Whether updating the minor version, changing the hardware definitions of the cluster or other adjustments that require a pod restart, CPO ensures that the update is as uninterrupted as possible.\nThis means that adjustments are carried out on the various pods of a particular cluster one after the other and in a sensible sequence. This happens as soon as a cluster consists of more than 1 PostgreSQL node.\nIn the event of a necessary restart, the operator independently stops the pods and does not leave this to Kubernetes. The idea behind this is that all replica pods are restarted one after the other first. The operator recognises these by the label cpo.opensource.cybertec.at/role=replica set by Patroni\nAs soon as all replicas are ready again, the operator checks whether one of the replicas is able to take over cluster operation and performs a switchover. Only then is the former leader pod stopped and restarted.\nThis ensures that the only effect on the application is the switchover. A completely uninterrupted handover of operation is not possible due to the architecture and connection handling of PostgreSQL. ","description":"Whether updating the minor version, changing the hardware definitions of the cluster or other adjustments that require a pod restart, CPO ensures that the update is as uninterrupted as possible.\nThis means that adjustments are carried out on the various pods of a particular cluster one after the other and in a sensible sequence. This happens as soon as a cluster consists of more than 1 PostgreSQL node.\nIn the event of a necessary restart, the operator independently stops the pods and does not leave this to Kubernetes. The idea behind this is that all replica pods are restarted one after the other first. The operator recognises these by the label cpo.opensource.cybertec.at/role=replica set by Patroni\n"},{"id":10,"href":"/CYBERTEC-pg-operator/backup/azure_blob/","title":"via Azure-Blob","parent":"Backup","content":"This chapter describes the use of pgBackRest in combination with Azure Blob Storage. It is not absolutely necessary to operate a Kubernetes on the Azure Cloud Platform. However, as with any cloud storage, the efficiency and therefore the duration of a backup depends on the connection.\nPrecondition: a blob-storage-volume and a priviledged role is needed for this chapter. Create a blob-storage-volume on the Azure console Create a priviledged service-role Modifying the Cluster As soon as all requirements are met:\nAn Azure-Blob-Storage-Volume A JSON token for the service role with the required authorisations for the Volume the cluster can be modified. Firstly, a secret containing the JSON token is created and the cluster manifest is adapted accordingly.\nThe first step is to create the required secret. This is most easily done using a kubectl command.\nkubectl create secret generic cluster-1-gcs-credentials --from-file=gcs.json=fluent.json In the next step, both the secret name and the file name of the JSON token are stored in the secret in the cluster manifest. In addition, global settings, such as the retention time of the backups in the global object, are defined, the image for pgBackRest is specified and the necessary information for the repository is added. This includes both the desired storage path in the bucket and the times for automatic backups based on the cron syntax.\napiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: backup: pgbackrest: configuration: secret: cluster-1-gcs-credentials global: repo1-path: /cluster-1/repo1/ repo1-retention-full: \u0026#39;7\u0026#39; repo1-retention-full-type: count image: docker.io/cybertecpostgresql/cybertec-pg-container:pgbackrest-16.4-1\u0026#39; repos: - name: repo1 resource: postgresql-backup-bucket key: gcs.json keyType: service schedule: full: 30 2 * * * storage: gcs ","description":"This chapter describes the use of pgBackRest in combination with Azure Blob Storage. It is not absolutely necessary to operate a Kubernetes on the Azure Cloud Platform. However, as with any cloud storage, the efficiency and therefore the duration of a backup depends on the connection.\nPrecondition: a blob-storage-volume and a priviledged role is needed for this chapter. Create a blob-storage-volume on the Azure console Create a priviledged service-role Modifying the Cluster As soon as all requirements are met:\n"},{"id":11,"href":"/CYBERTEC-pg-operator/backup/encryption/","title":"Encrypted Backups","parent":"Backup","content":"pgBackRest also allows you to encrypt your backups on the client side before uploading them. This is possible with any type of storage and is very easy to activate.\nFirstly, we need to define an encryption key. This must be specified separately for each repo and stored in the same secret that is defined in the spec.backup.pgbackrest.configuration.secret object.\nkind: Secret apiVersion: v1 metadata: name: cluster-1-s3-credential namespace: cpo stringData: s3.conf | [global] repo1-s3-key=YOUR_S3_KEY repo1-s3-key-secret=YOUR_S3_KEY_SECRET repo1-cipher-pass=YOUR_ENCRYPTION_KEY We also need to configure the type of encryption for pgBackRest. This is done via the cipher-type parameter, which must also be specified for each repo. You can find the available values for the parameter here\napiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster namespace: cpo spec: backup: pgbackrest: configuration: secret: cluster-1-s3-credential global: repo1-path: /cluster/repo1/ repo1-retention-full: \u0026#39;7\u0026#39; repo1-retention-full-type: count repo1-cipher-type: aes-256-cbc image: \u0026#39;docker.io/cybertecpostgresql/cybertec-pg-container:pgbackrest-16.4-1\u0026#39; repos: - endpoint: \u0026#39;https://s3-zurich.cyberlink.cloud:443\u0026#39; name: repo1 region: zurich resource: cpo-cluster-bucket schedule: full: 30 2 * * * incr: \u0026#39;*/30 * * * *\u0026#39; storage: s3 ","description":"pgBackRest also allows you to encrypt your backups on the client side before uploading them. This is possible with any type of storage and is very easy to activate.\nFirstly, we need to define an encryption key. This must be specified separately for each repo and stored in the same secret that is defined in the spec.backup.pgbackrest.configuration.secret object.\nkind: Secret apiVersion: v1 metadata: name: cluster-1-s3-credential namespace: cpo stringData: s3.conf | [global] repo1-s3-key=YOUR_S3_KEY repo1-s3-key-secret=YOUR_S3_KEY_SECRET repo1-cipher-pass=YOUR_ENCRYPTION_KEY We also need to configure the type of encryption for pgBackRest. This is done via the cipher-type parameter, which must also be specified for each repo. You can find the available values for the parameter here\n"},{"id":12,"href":"/CYBERTEC-pg-operator/backup/check_backups/","title":"Check/Monitor Backups","parent":"Backup","content":"There are several ways to gain an insight into the current status of pgBackRest. One of these is to use pgBackRest within the container. This can be done both via the repo host and the Postgres pod.\npgbackrest via terminal (Repo-Host-Pod) kubectl exec cluster-5-pgbackrest-repo-host-0 --stdin --tty -- pgbackrest info stanza: db status: ok cipher: none db (current) wal archive min/max (16): 00000006000000000000005C/000000070000000000000092 full backup: 20240517-125730F timestamp start/stop: 2024-05-17 12:57:30+00 / 2024-05-17 12:57:41+00 wal start/stop: 00000007000000000000005E / 00000007000000000000005E database size: 22.9MB, database backup size: 22.9MB repo1: backup set size: 3MB, backup size: 3MB incr backup: 20240517-125730F_20240517-130003I timestamp start/stop: 2024-05-17 13:00:03+00 / 2024-05-17 13:00:05+00 wal start/stop: 000000070000000000000060 / 000000070000000000000060 database size: 22.9MB, database backup size: 904.3KB repo1: backup set size: 3MB, backup size: 149.4KB backup reference list: 20240517-125730F incr backup: 20240517-125730F_20240517-131503I timestamp start/stop: 2024-05-17 13:15:03+00 / 2024-05-17 13:15:04+00 wal start/stop: 000000070000000000000062 / 000000070000000000000062 database size: 22.9MB, database backup size: 24.3KB repo1: backup set size: 3MB, backup size: 2.9KB backup reference list: 20240517-125730F, 20240517-125730F_20240517-130003I pgbackrest via terminal (Postgres-Pod) kubectl exec cluster-5-0 --stdin --tty -- pgbackrest info Defaulted container \u0026#34;postgres\u0026#34; out of: postgres, postgres-exporter, pgbackrest-restore (init) stanza: db status: ok cipher: none db (current) wal archive min/max (16): 00000006000000000000005C/000000070000000000000092 full backup: 20240517-125730F timestamp start/stop: 2024-05-17 12:57:30+00 / 2024-05-17 12:57:41+00 wal start/stop: 00000007000000000000005E / 00000007000000000000005E database size: 22.9MB, database backup size: 22.9MB repo1: backup set size: 3MB, backup size: 3MB incr backup: 20240517-125730F_20240517-130003I timestamp start/stop: 2024-05-17 13:00:03+00 / 2024-05-17 13:00:05+00 wal start/stop: 000000070000000000000060 / 000000070000000000000060 database size: 22.9MB, database backup size: 904.3KB repo1: backup set size: 3MB, backup size: 149.4KB backup reference list: 20240517-125730F incr backup: 20240517-125730F_20240517-131503I timestamp start/stop: 2024-05-17 13:15:03+00 / 2024-05-17 13:15:04+00 wal start/stop: 000000070000000000000062 / 000000070000000000000062 database size: 22.9MB, database backup size: 24.3KB repo1: backup set size: 3MB, backup size: 2.9KB backup reference list: 20240517-125730F, 20240517-125730F_20240517-130003I There is the \u0026ldquo;normal\u0026rdquo; output, as well as the output format Json, which can be processed directly in the terminal.\nkubectl exec cluster-5-0 --stdin --tty -- pgbackrest info --output=json Check pgBackrest via Monitoring In addition to reading the status via the containers, pgBackRest can also be analysed and monitored via the monitoring stack. You can find information on setting up the monitoring stack and further information here.\n","description":"There are several ways to gain an insight into the current status of pgBackRest. One of these is to use pgBackRest within the container. This can be done both via the repo host and the Postgres pod.\npgbackrest via terminal (Repo-Host-Pod) kubectl exec cluster-5-pgbackrest-repo-host-0 --stdin --tty -- pgbackrest info stanza: db status: ok cipher: none db (current) wal archive min/max (16): 00000006000000000000005C/000000070000000000000092 full backup: 20240517-125730F timestamp start/stop: 2024-05-17 12:57:30+00 / 2024-05-17 12:57:41+00 wal start/stop: 00000007000000000000005E / 00000007000000000000005E database size: 22.9MB, database backup size: 22.9MB repo1: backup set size: 3MB, backup size: 3MB incr backup: 20240517-125730F_20240517-130003I timestamp start/stop: 2024-05-17 13:00:03+00 / 2024-05-17 13:00:05+00 wal start/stop: 000000070000000000000060 / 000000070000000000000060 database size: 22.9MB, database backup size: 904.3KB repo1: backup set size: 3MB, backup size: 149.4KB backup reference list: 20240517-125730F incr backup: 20240517-125730F_20240517-131503I timestamp start/stop: 2024-05-17 13:15:03+00 / 2024-05-17 13:15:04+00 wal start/stop: 000000070000000000000062 / 000000070000000000000062 database size: 22.9MB, database backup size: 24.3KB repo1: backup set size: 3MB, backup size: 2.9KB backup reference list: 20240517-125730F, 20240517-125730F_20240517-130003I pgbackrest via terminal (Postgres-Pod) kubectl exec cluster-5-0 --stdin --tty -- pgbackrest info Defaulted container \u0026#34;postgres\u0026#34; out of: postgres, postgres-exporter, pgbackrest-restore (init) stanza: db status: ok cipher: none db (current) wal archive min/max (16): 00000006000000000000005C/000000070000000000000092 full backup: 20240517-125730F timestamp start/stop: 2024-05-17 12:57:30+00 / 2024-05-17 12:57:41+00 wal start/stop: 00000007000000000000005E / 00000007000000000000005E database size: 22.9MB, database backup size: 22.9MB repo1: backup set size: 3MB, backup size: 3MB incr backup: 20240517-125730F_20240517-130003I timestamp start/stop: 2024-05-17 13:00:03+00 / 2024-05-17 13:00:05+00 wal start/stop: 000000070000000000000060 / 000000070000000000000060 database size: 22.9MB, database backup size: 904.3KB repo1: backup set size: 3MB, backup size: 149.4KB backup reference list: 20240517-125730F incr backup: 20240517-125730F_20240517-131503I timestamp start/stop: 2024-05-17 13:15:03+00 / 2024-05-17 13:15:04+00 wal start/stop: 000000070000000000000062 / 000000070000000000000062 database size: 22.9MB, database backup size: 24.3KB repo1: backup set size: 3MB, backup size: 2.9KB backup reference list: 20240517-125730F, 20240517-125730F_20240517-130003I There is the \u0026ldquo;normal\u0026rdquo; output, as well as the output format Json, which can be processed directly in the terminal.\n"},{"id":13,"href":"/CYBERTEC-pg-operator/project/","title":"CPO","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":14,"href":"/CYBERTEC-pg-operator/project/project/","title":"The Project","parent":"CPO","content":"The CYBERTEC PostgreSQL Operator (CPO) enables the simple provision and management of PostgreSQL clusters on Kubernetes. It reduces the administration effort and facilitates the management of single-node and HA clusters.\nMain components CYBERTEC-pg-operator: Kubernetes operator for the automation of PostgreSQL clusters. CYBERTEC-pg-container: Docker container suite for PostgreSQL, Patroni and etcd for the provision of HA clusters. CYBERTEC-operator-tutorials: Tutorials and instructions for installing and using the operator. Features Cluster management: Single-node and HA (High Availability) clusters via Patroni Reduction of downtime thanks to redundancy, pod anti-affinity, auto-failover and self-healing Automated failover Live volume resize without pod restarts Basic credential and user management on K8s, eases application deployments Compatible with OpenShift and Rancher PostgreSQL compatibility: Supports PostgreSQL versions 13 to 17 Inplace upgrades for smooth version changes and minimal downtime Extensive extension support, including pgAudit, TimescaleDB and PostGIS Standby-Cluster Backup \u0026amp; Restore: Integrated pgBackRest support Automatic backups Point-in-Time- and Snapshot-based Restores / Disaster Recovery Connection management: pgBouncer for connection pooling Monitoring \u0026amp; alerting stack Integrated metrics exporter Prometheus, alert manager for metrics collection and alerting Grafana for visual monitoring of the clusters Operator UI: Web interface for managing clusters Installation Detailed instructions on installation and configuration can be found in the CYBERTEC operator tutorials and in the following chapters Example of installation via Helm:\nhelm repo add cybertec https://cybertec-postgresql.github.io/helm-charts/ helm install pg-operator cybertec/cybertec-pg-operator More information: Installation\nContribution This project is open source, and contributions to its further development are expressly encouraged. Possible forms of contribution:\nBug reports and feature requests Code contributions (pull requests welcome) Improvement of the documentation Further details on contributions can be found in the respective GitHub repositories. Licence The CYBERTEC PostgreSQL Operator is licensed under the Apache 2.0 licence.\n","description":"The CYBERTEC PostgreSQL Operator (CPO) enables the simple provision and management of PostgreSQL clusters on Kubernetes. It reduces the administration effort and facilitates the management of single-node and HA clusters.\nMain components CYBERTEC-pg-operator: Kubernetes operator for the automation of PostgreSQL clusters. CYBERTEC-pg-container: Docker container suite for PostgreSQL, Patroni and etcd for the provision of HA clusters. CYBERTEC-operator-tutorials: Tutorials and instructions for installing and using the operator. Features Cluster management: Single-node and HA (High Availability) clusters via Patroni Reduction of downtime thanks to redundancy, pod anti-affinity, auto-failover and self-healing Automated failover Live volume resize without pod restarts Basic credential and user management on K8s, eases application deployments Compatible with OpenShift and Rancher PostgreSQL compatibility: Supports PostgreSQL versions 13 to 17 Inplace upgrades for smooth version changes and minimal downtime Extensive extension support, including pgAudit, TimescaleDB and PostGIS Standby-Cluster Backup \u0026amp; Restore: Integrated pgBackRest support Automatic backups Point-in-Time- and Snapshot-based Restores / Disaster Recovery Connection management: pgBouncer for connection pooling Monitoring \u0026amp; alerting stack Integrated metrics exporter Prometheus, alert manager for metrics collection and alerting Grafana for visual monitoring of the clusters Operator UI: Web interface for managing clusters Installation Detailed instructions on installation and configuration can be found in the CYBERTEC operator tutorials and in the following chapters Example of installation via Helm:\n"},{"id":15,"href":"/CYBERTEC-pg-operator/project/container_images/","title":"Container Images","parent":"CPO","content":"For each version of the operator and the required PostgreSQL and other required containers, the corresponding image is provided on Dockerhub.\nOperator container images The operator images are the central components that control the operation and administration of the PostgreSQL databases. These images are available in the following repository on DockerHub:\nOperator Images\nThe repository contains all the necessary images for running the Cybertec PG Operator in a Kubernetes environment. These images are updated regularly to ensure the latest features and security updates.\nAdditional container images In addition to the operator images, various container images are required to support a complete PostgreSQL environment. These images are available in the following repository: CYBERTEC-PG-Container Images\nThis repository contains images for the following components:\nPostgreSQL: The main database image, which contains all supported major versions of PostgreSQL. The name of the tag always reflects the latest release, e.g. currently 16.4 for PostgreSQL 16.4. For the other major versions, the corresponding minor versions released by the PostgreSQL community at the same time are included. Postgres-GIS: A specialised image that combines PostgreSQL with the PostGIS extension to support spatial data processing functions. You can find more information about Postgis here.\nThe tag for Postgis also includes the included version of Postgis. Example: postgres-16.4-34-1 Postgis: 3.4.x PGBackRest: A backup and restore tool developed specifically for PostgreSQL and available as a separate container image. Exporter: Images for monitoring PostgreSQL databases that collect metrics and make them available for monitoring tools such as Prometheus. PgBouncer: A lightweight connection pooler for PostgreSQL that manages and optimises the number of concurrent connections. Extensions You can view the versions included in the Extensions section.\n","description":"For each version of the operator and the required PostgreSQL and other required containers, the corresponding image is provided on Dockerhub.\nOperator container images The operator images are the central components that control the operation and administration of the PostgreSQL databases. These images are available in the following repository on DockerHub:\n"},{"id":16,"href":"/CYBERTEC-pg-operator/architecture/","title":"Architecture","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":17,"href":"/CYBERTEC-pg-operator/crd/crd-postgresql/","title":"PostgreSQL","parent":"References","content":" postgresql Name Type required Description apiVersion string true acid.zalando.do/v1 kind string true metadata object true spec object true status object false spec Name Type required Description backup object false Enables the definition of a customised backup solution for the cluster teamId string true name of the team the cluster belongs to numberOfInstances Int true Number of nodes of the cluster dockerImages string false Define a custom image to override the default schedulerName string false Define a custom Name to override the default spiloRunAsUser string false Define an User id which should be used for the pods spiloRunAsGroup string false Define an Group id which should be used for the pods spiloFSGroup string false Persistent Volumes for the pods in the StatefulSet will be owned and writable by the group ID specified. enableMasterLoadBalancer boolean false Define whether to enable the load balancer pointing to the Postgres primary enableMasterPoolerLoadBalancer boolean false Define whether to enable the load balancer pointing to the primary ConnectionPooler enableReplicaLoadBalancer boolean false Define whether to enable the load balancer pointing to the Postgres replicas enableReplicaPoolerLoadBalancer boolean false Define whether to enable the load balancer pointing to the Replica-ConnectionPooler allowedSourceRange string false Defines the range of IP networks (in CIDR-notation). The corresponding load balancer is accessible only to the networks defined by this parameter. users map false a map of usernames to user flags for the users that should be created in the cluster by the operator usersWithSecretRotation list false list of users to enable credential rotation in K8s secrets. The rotation interval can only be configured globally. usersWithInPlaceSecretRotation list false list of users to enable in-place password rotation in K8s secrets. The rotation interval can only be configured globally. databases map false a map of databases that should be created in the cluster by the operator tolerations list false a list of tolerations that apply to the cluster pods. Each element of that list is a dictionary with the following fields: key, operator, value, effect and tolerationSeconds podPriorityClassName string false a name of the priority class that should be assigned to the cluster pods. If not set then the default priority class is taken. The priority class itself must be defined in advance podAnnotations map false A map of key value pairs that gets attached as annotations to each pod created for the database. ServiceAnnotations map false A map of key value pairs that gets attached as annotations to each Service created for the database. enableShmVolume boolean false Start a database pod without limitations on shm memory. By default Docker limit /dev/shm to 64M (see e.g. the docker issue, which could be not enough if PostgreSQL uses parallel workers heavily. If this option is present and value is true, to the target database pod will be mounted a new tmpfs volume to remove this limitation. enableConnectionPooler boolean false creates a ConnectionPooler for the primary Database enableReplicaConnectionPooler boolean false creates a ConnectionPooler for the replica Databases enableLogicalBackup boolean false Enable logical Backups for this Cluster (Stored on S3) - s3-configuration for Operator is needed (Not for pgBackRest) logicalBackupSchedule string false Schedule for the logical backup K8s cron job. (Not for pgBackRest) additionalVolumes list false List of additional volumes to mount in each container of the statefulset pod. Each item must contain a name, mountPath, and volumeSource which is a kubernetes volumeSource. It allows you to mount existing PersistentVolumeClaims, ConfigMaps and Secrets inside the StatefulSet. back to parent\nbackup Name Type required Description pgbackrest object false Enables the definition of a pgbackrest-setup for the cluster back to parent\npgbackrest Name Type required Description configuration object false Enables the definition of a pgbackrest-setup for the cluster global object false image string true repos array true resources: object false Resource definition (limits.cpu, limits.memory \u0026amp; requests.cpu \u0026amp; requests.memory) back to parent\nconfiguration Name Type required Description secret object false Secretname with the contained S3 credentials (AccessKey \u0026amp; SecretAccessKey) (Note: must be placed in the same namespace as the cluster) protection object false Enable Protection-Options back to parent\nprotection Name Type required Description restore boolean false A restore is ignored as long as this option is set to true. back to parent\nrepos Name Type required Description name string true Name of the repository Required:Repo[1-4] storage string true Defines the used backup-storage (Choose from List: pvc,s3,blob,gcs) resource string true Bucket-/Instance-/Storage- or PVC-Name endpoint string false The Endpoint for the choosen Storage (Not required for local storage) region string false Region for the choosen Storage (S3 only) schedule string false Object for defining automatic backups back to parent\nschedule Name Type required Description full string false (Cron-Syntax) Define full backup incr string false (Cron-Syntax) Define incremental backup diff string false (Cron-Syntax) Define differential backup back to parent\nstatus Name Type required Description PostgresClusterStatus string false Shows the cluster status. Filled by the Operator back to parent\n","description":" postgresql Name Type required Description apiVersion string true acid.zalando.do/v1 kind string true metadata object true spec object true status object false spec Name Type required Description backup object false Enables the definition of a customised backup solution for the cluster teamId string true name of the team the cluster belongs to numberOfInstances Int true Number of nodes of the cluster dockerImages string false Define a custom image to override the default schedulerName string false Define a custom Name to override the default spiloRunAsUser string false Define an User id which should be used for the pods spiloRunAsGroup string false Define an Group id which should be used for the pods spiloFSGroup string false Persistent Volumes for the pods in the StatefulSet will be owned and writable by the group ID specified. enableMasterLoadBalancer boolean false Define whether to enable the load balancer pointing to the Postgres primary enableMasterPoolerLoadBalancer boolean false Define whether to enable the load balancer pointing to the primary ConnectionPooler enableReplicaLoadBalancer boolean false Define whether to enable the load balancer pointing to the Postgres replicas enableReplicaPoolerLoadBalancer boolean false Define whether to enable the load balancer pointing to the Replica-ConnectionPooler allowedSourceRange string false Defines the range of IP networks (in CIDR-notation). The corresponding load balancer is accessible only to the networks defined by this parameter. users map false a map of usernames to user flags for the users that should be created in the cluster by the operator usersWithSecretRotation list false list of users to enable credential rotation in K8s secrets. The rotation interval can only be configured globally. usersWithInPlaceSecretRotation list false list of users to enable in-place password rotation in K8s secrets. The rotation interval can only be configured globally. databases map false a map of databases that should be created in the cluster by the operator tolerations list false a list of tolerations that apply to the cluster pods. Each element of that list is a dictionary with the following fields: key, operator, value, effect and tolerationSeconds podPriorityClassName string false a name of the priority class that should be assigned to the cluster pods. If not set then the default priority class is taken. The priority class itself must be defined in advance podAnnotations map false A map of key value pairs that gets attached as annotations to each pod created for the database. ServiceAnnotations map false A map of key value pairs that gets attached as annotations to each Service created for the database. enableShmVolume boolean false Start a database pod without limitations on shm memory. By default Docker limit /dev/shm to 64M (see e.g. the docker issue, which could be not enough if PostgreSQL uses parallel workers heavily. If this option is present and value is true, to the target database pod will be mounted a new tmpfs volume to remove this limitation. enableConnectionPooler boolean false creates a ConnectionPooler for the primary Database enableReplicaConnectionPooler boolean false creates a ConnectionPooler for the replica Databases enableLogicalBackup boolean false Enable logical Backups for this Cluster (Stored on S3) - s3-configuration for Operator is needed (Not for pgBackRest) logicalBackupSchedule string false Schedule for the logical backup K8s cron job. (Not for pgBackRest) additionalVolumes list false List of additional volumes to mount in each container of the statefulset pod. Each item must contain a name, mountPath, and volumeSource which is a kubernetes volumeSource. It allows you to mount existing PersistentVolumeClaims, ConfigMaps and Secrets inside the StatefulSet. back to parent\n"},{"id":18,"href":"/CYBERTEC-pg-operator/crd/crd-operator-configurator/","title":"Operator-Configuration","parent":"References","content":" Name Type default Description enable_crd_registration boolean true crd_categories string all enable_lazy_spilo_upgrade boolean false enable_pgversion_env_var boolean true enable_spilo_wal_path_combat boolean false etcd_host string kubernetes_use_configmaps boolean false docker_image string sidecars list enable_shm_volume boolean true workers int 8 max_instances int -1 min_instances int -1 resync_period string 30m repair_period string 5m set_memory_request_to_limit boolean false debug_logging boolean true enable_db_access boolean true spilo_privileged boolean false spilo_allow_privilege_escalation boolean true watched_namespace string * major-upgrade-specific Name Type default Description major_version_upgrade_mode string off major_version_upgrade_team_allow_list string minimal_major_version string 9.6 target_major_version string 14 aws-specific Name Type default Description wal_s3_bucket string log_s3_bucket string kube_iam_role string aws_region string additional_secret_mount string additional_secret_mount_path string enable_ebs_gp3_migration boolean enable_ebs_gp3_migration_max_size int logical-backup-specific Name Type default Description logical_backup_docker_image string logical_backup_google_application_credentials string logical_backup_job_prefix string logical_backup_provider string logical_backup_s3_access_key_id string logical_backup_s3_bucket string logical_backup_s3_endpoint string logical_backup_s3_region string logical_backup_s3_secret_access_key string logical_backup_s3_sse string logical_backup_s3_retention_time string logical_backup_schedule string (Cron-Syntax) team-api-specific Name Type default Description enable_teams_api string teams_api_url string teams_api_role_configuration string enable_team_superuser boolean team_admin_role boolean enable_admin_role_for_users boolean pam_role_name string pam_configuration string protected_role_names list postgres_superuser_teams string role_deletion_suffix string enable_team_member_deprecation boolean enable_postgres_team_crd boolean enable_postgres_team_crd_superusers boolean enable_team_id_clustername_prefix boolean ","description":" Name Type default Description enable_crd_registration boolean true crd_categories string all enable_lazy_spilo_upgrade boolean false enable_pgversion_env_var boolean true enable_spilo_wal_path_combat boolean false etcd_host string kubernetes_use_configmaps boolean false docker_image string sidecars list enable_shm_volume boolean true workers int 8 max_instances int -1 min_instances int -1 resync_period string 30m repair_period string 5m set_memory_request_to_limit boolean false debug_logging boolean true enable_db_access boolean true spilo_privileged boolean false spilo_allow_privilege_escalation boolean true watched_namespace string * major-upgrade-specific Name Type default Description major_version_upgrade_mode string off major_version_upgrade_team_allow_list string minimal_major_version string 9.6 target_major_version string 14 aws-specific Name Type default Description wal_s3_bucket string log_s3_bucket string kube_iam_role string aws_region string additional_secret_mount string additional_secret_mount_path string enable_ebs_gp3_migration boolean enable_ebs_gp3_migration_max_size int logical-backup-specific Name Type default Description logical_backup_docker_image string logical_backup_google_application_credentials string logical_backup_job_prefix string logical_backup_provider string logical_backup_s3_access_key_id string logical_backup_s3_bucket string logical_backup_s3_endpoint string logical_backup_s3_region string logical_backup_s3_secret_access_key string logical_backup_s3_sse string logical_backup_s3_retention_time string logical_backup_schedule string (Cron-Syntax) team-api-specific Name Type default Description enable_teams_api string teams_api_url string teams_api_role_configuration string enable_team_superuser boolean team_admin_role boolean enable_admin_role_for_users boolean pam_role_name string pam_configuration string protected_role_names list postgres_superuser_teams string role_deletion_suffix string enable_team_member_deprecation boolean enable_postgres_team_crd boolean enable_postgres_team_crd_superusers boolean enable_team_id_clustername_prefix boolean "},{"id":19,"href":"/CYBERTEC-pg-operator/quickstart/","title":"Quickstart","parent":"CPO (CYBERTEC-PG-Operator)","content":"We can tell and document so much about our project but it seems you just want to get started. Let us show you the fastest way to use CPO.\nPreconditions git helm (optional) kubectl or oc Let\u0026rsquo;s start Step 1 - Preparations To get started, you can fork our tutorial repository on Github and then download it. CYBERTEC-operator-tutorials\ngit clone https://github.com/cybertec-postgresql/CYBERTEC-operator-tutorials.git cd CYBERTEC-operator-tutorials Step 2 - Install the Operator Two options are available for the installation:\nInstallation via Helm-Chart Installation via apply Installation via Helm-Chart kubectl apply -n cpo -k setup/namespace/. helm install cpo -n cpo setup/helm/operator/ Installation via apply kubectl apply -n cpo -k setup/namespace/. kubectl apply -n cpo -k setup/helm/operator/. You can check if the operator pod is in operation.\nkubectl get pods -n cpo --selector=cpo.cybertec.at/pod/type=postgres-operator The result should look like this:\nNAME READY STATUS RESTARTS AGE postgres-operator-599688d948-fw8pw 1/1 Running 0 41s The operator is ready and the setup is complete. The next step is the creation of a Postgres cluster\nStep 3 - Create a Cluster To create a simple cluster, the following command is sufficient\nkubectl apply -n cpo -f cluster-tutorials/single-cluster watch kubectl get pods -n cpo --selector cluster-name=cluster-1 The result should look like this:\nAlle 2.0s: kubectl get pods -n cpo --selector cluster-name=cluster-1 NAME READY STATUS RESTARTS AGE cluster-1-0 2/2 Running 0 28s cluster-1-1 0/2 PodInitializing 0 9s Step 4 - Connect to the Database Get your login information from the secret.\nkubectl get secret -n cpo postgres.cluster-1.credentials.postgresql.cpo.opensource.cybertec.at -o jsonpath=\u0026#39;{.data}\u0026#39; | jq \u0026#39;.|map_values(@base64d)\u0026#39; The result should look like this:\n{ \u0026#34;password\u0026#34;: \u0026#34;2rZG1Kx9asdHscswQGzff4Ru0xW6uasacy3GQ0sjdCH3wWr0kguUXUZek6dkemsf\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;postgres\u0026#34; } Connection via port-forward kubectl port-forward -n cpo cluster-1-0 5432:5432 # using psql PGPASSWORD=2rZG1Kx9asdHscswQGzffjdCH3wWr0kguUXUZek6dkemsf psql -h 127.0.0.1 -p 5432 -U postgres # using usql PGPASSWORD=2rZG1Kx9asdHscswQGzffjdCH3wWr0kguUXUZek6dkemsf usql postgresql://postgres@127.0.0.1/postgres Next Steps Congratulations, your first cluster is ready and you were able to connect to it. On the following pages we have put together an introduction with lots of information and details to show you the different possibilities and components of CPO.\n","description":"We can tell and document so much about our project but it seems you just want to get started. Let us show you the fastest way to use CPO.\nPreconditions git helm (optional) kubectl or oc Let\u0026rsquo;s start Step 1 - Preparations To get started, you can fork our tutorial repository on Github and then download it. CYBERTEC-operator-tutorials\n"},{"id":20,"href":"/CYBERTEC-pg-operator/installation/","title":"Installation","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":21,"href":"/CYBERTEC-pg-operator/installation/dev-k8s/","title":"Setup local Kubernetes","parent":"Installation","content":"There are various options for setting up a local Kubernetes environment. This chapter deals with the following two variants:\nminikube crc (CodeReadyContainers from RedHat) Minikube Minikube is a tool that makes it possible to run Kubernetes locally on a single computer. It sets up a minimal but functional Kubernetes environment suitable for development and testing purposes. Minikube supports most Kubernetes features and provides an easy way to launch and manage Kubernetes clusters on local machines without the need for a complex cloud infrastructure.\nInstall Kubectl \u0026amp; Minikube To use Minikube, it is essential to install the Kubectl client.\nHere you will find all the information you need to install kubectl on your Linux, Mac or Windows device.\nYou can Install Minikube on your Linux-, Mac- or Windows-Devide using this Documentation.\nUse Minikube Before starting minikube, it is advisable to define a path for the kubeconfig.\nexport KUBECONFIG=/home/USERNAME/kubeconfig_minikube.conf You can then start minikube and all the necessary data is written directly to the conf. The definition of a user-defined path ensures that other configs are not inadvertently overwritten. The path must be defined again via ENV in each new user session. Alternatively, this can also be permanently defined via .bashrc. If the default path is not used for any other purpose, the ENV does not need to be set.\n# Start minikube minikube start # get pods from default namespace kubectl get pods # change default namespace to cpo kubectl config set-context --namespace=cpo CRC CRC (CodeReady Containers) is a tool from Red Hat that provides a local OpenShift environment. It is specifically designed to run a compact version of OpenShift on a local machine to provide developers and testers with an easy way to develop and test applications optimised for use in OpenShift. CRC includes all the necessary OpenShift components and makes it possible to use Red Hat\u0026rsquo;s container platform locally without building a full cloud infrastructure.\nInstall oc-client \u0026amp; CRC To use CRC, it is essential to install the oc-client or the kubectl-client.\nHere you will find all the information you need to install kubectl on your Linux, Mac or Windows device.\nYou can Download and install CRC on your Linux-, Mac- or Windows-Devide using this informations.\nUse CRC Before installing crc, it is advisable to define a path for the kubeconfig.\nexport KUBECONFIG=/home/USERNAME/kubeconfig_crc.conf You can then install and start crc and all the necessary data is written directly to the conf. The definition of a user-defined path ensures that other configs are not inadvertently overwritten. The path must be defined again via ENV in each new user session. Alternatively, this can also be permanently defined via .bashrc. If the default path is not used for any other purpose, the ENV does not need to be set.\n# Install crc crc setup # Start crc crc start # get pods from default namespace oc get pods # change default namespace to cpo oc project cpo ","description":"There are various options for setting up a local Kubernetes environment. This chapter deals with the following two variants:\nminikube crc (CodeReadyContainers from RedHat) Minikube Minikube is a tool that makes it possible to run Kubernetes locally on a single computer. It sets up a minimal but functional Kubernetes environment suitable for development and testing purposes. Minikube supports most Kubernetes features and provides an easy way to launch and manage Kubernetes clusters on local machines without the need for a complex cloud infrastructure.\n"},{"id":22,"href":"/CYBERTEC-pg-operator/installation/install_operator/","title":"Install CPO","parent":"Installation","content":" Prerequisites For the installation you either need our CPO tutorial repository or you install CPO directly from our registry. Exception: Installation via Operatorhub (Openshift only)\nCPO-Tutorial-Repository To get started, you can fork our tutorial repository on Github and then download it. CYBERTEC-operator-tutorials\nGITHUB_USER=\u0026#39;[YOUR_USERNAME]\u0026#39; git clone https://github.com/$GITHUB_USER/CYBERTEC-operator-tutorials.git cd CYBERTEC-operator-tutorials Helm-Registry helm repo add cpo https://cybertec-postgresql.github.io/CYBERTEC-operator-tutorials\nCreate Namespace # kubectl kubectl create namespace cpo # oc oc create namespace cpo Install CPO There are several ways to install CPO:\nUse Helm Use apply Use Operatorhub (On Openshift only) Helm You can check and change the value.yaml of the helm diagram under the path helm/operator/values.yaml By default, the operator is defined so that it is configured via crd-configuration. If you wish, you can change this to configmap. There are also some other default settings.\nhelm install -n cpo cpo helm/operator/. The installation uses a standard configuration. On the following page you will find more information on how to configure cpo and thus adapt it to your requirements.\nApply The installation uses a standard configuration. On the following page you will find more information on how to configure cpo and thus adapt it to your requirements.\nOperatorhub The installation uses a standard configuration. On the following page you will find more information on how to configure cpo and thus adapt it to your requirements.\n","description":" Prerequisites For the installation you either need our CPO tutorial repository or you install CPO directly from our registry. Exception: Installation via Operatorhub (Openshift only)\nCPO-Tutorial-Repository To get started, you can fork our tutorial repository on Github and then download it. CYBERTEC-operator-tutorials\n"},{"id":23,"href":"/CYBERTEC-pg-operator/installation/configuration_operator/","title":"Operator-Configuration","parent":"Installation","content":"Users who are already used to working with PostgreSQL from Baremetal or VMs are already familiar with the need for various files to configure PostgreSQL. These include\npostgresql.conf pg_hba.conf \u0026hellip; Although these files are available in the container, direct modification is not planned. As part of the declarative mode of operation of the operator, these files are defined via the operator. The modifying intervention within the container also represents a contradiction to the immutability of the container.\nFor these reasons, the operator provides a way to make adjustments to the various files, from PostgreSQL to Patroni.\nWe differentiate between two main objects in the cluster manifest:\npostgresql with the child objects version and parameters patroni with objects for the pg_hab, slots and much more postgresql The postgresql object consists of the following elements:\nversion - allows you to select the major version of PostgreSQL used. parameters- enables the postgresql.conf to be changed spec: postgresql: parameters: shared_preload_libraries: \u0026#39;pg_stat_statements,pgnodemx, timescaledb\u0026#39; shared_buffers: \u0026#39;512MB\u0026#39; version: \u0026#39;16\u0026#39; Any known PostgreSQL parameter from postgresql.conf can be entered here and will be delivered by the operator to all nodes of the cluster accordingly.\nYou can find more information about the parameters in the PostgreSQL documentation\npatroni The patroni object contains numerous options for customising the patroni-setu, and the pg_hba.conf is also configured here. A complete list of all available elements can be found here.\nThe most important elements include\npg_hba - pg_hba.conf slots synchronous_mode - enables synchronous mode in the cluster. The default is set to false maximum_lag_on_failover - Specifies the maximum lag so that the pod is still considered healthy in the event of a failover. failsafe_mode Allows you to cancel the downgrading of the leader if all cluster members can be reached via the Patroni Rest Api. You can find more information on this in the Patroni documentation pg_hba The pg_hba.conf contains all defined authentication rules for PostgreSQL.\nWhen customising this configuration, it is important that the entire version of pg_hba is written to the manifest. The current configuration can be read out in the database using table pg_hba_file_rules ;.\nFurther information can be found in the PostgreSQL documentation\nslots When using user-defined slots, for example for the use of CDC using Debezium, there are problems when interacting with Patroni, as the slot and its current status are not automatically synchronised to the replicas.\nIn the event of a failover, the client cannot start replication as both the entire slot and the information about the data that has already been synchronised are missing.\nTo resolve this problem, slots must be defined in the cluster manifest rather than in PostgreSQL.\nspec: patroni: slots: cdc-example: database: app_db plugin: pgoutput type: logical This example creates a logical replication slot with the name cdc-example within the app_db database and uses the pgoutput plugin for the slot.\nATTENTION: Slots are only synchronised from the leader/standby leader to the replicas. This means that using the slots read-only on the replicas will cause a problem in the event of a failover.\n","description":"Users who are already used to working with PostgreSQL from Baremetal or VMs are already familiar with the need for various files to configure PostgreSQL. These include\npostgresql.conf pg_hba.conf \u0026hellip; Although these files are available in the container, direct modification is not planned. As part of the declarative mode of operation of the operator, these files are defined via the operator. The modifying intervention within the container also represents a contradiction to the immutability of the container.\n"},{"id":24,"href":"/CYBERTEC-pg-operator/first_cluster/","title":"Create a Cluster","parent":"CPO (CYBERTEC-PG-Operator)","content":"To set up a cluster, the implementation is based on a description, as with the other Kubernetes deplyoments. To do this, the operator uses a document of type postgresql.\nYou can also find the basic minimum specifications for a single-node cluster in our tutorial project on Github\nminimal Single-Node Cluster apiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: dockerImage: \u0026#34;docker.io/cybertecpostgresql/cybertec-pg-container:postgres-16.4-1\u0026#34; numberOfInstances: 1 postgresql: version: \u0026#34;16\u0026#34; resources: limits: cpu: 500m memory: 500Mi requests: cpu: 500m memory: 500Mi volume: size: 5Gi Based on this Manifest the Operator will deploy a single-Node-Cluster based on the defined dockerImage and start the included Postgres-16-Server. Also created is a volume based on your default-storage Class. The Ressource-Definiton means, that we reserve a half cpu and a half GB Memory for this Cluster with the same Definition as limit.\nAfter some seconds we should see, that the operator creates our cluster based on the declared definitions.\nkubectl get pods ----------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cluster-1-0 | 1/1 | Running | 0 | 50s HINT: Here you will find a complete overview of the available options within the cluster manifest.\nUse a specific Storageclass spec: ... volume: size: 5Gi storageClass: default-provisioner ... Using the storageClass-Definiton allows us to define a specific storageClass for this Cluster. Please ensure, that the storageClass exists and is usable. If a Volume cannot provide the Volume will stand in the pending-State as like the Database-Pod.\nExpanding Volume The Operator allows to you expand your volume if the storage-System is able to do this.\nspec: ... volume: size: 10Gi storageClass: default-provisioner ... This will trigger the expand of your Cluster-Volumes. It will need some time and you can check the current state inside the pvc.\nkubectl get pvc pgdata-cluster-1-0 -o yaml ------------------------------------------------------- spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: crc-csi-hostpath-provisioner volumeMode: Filesystem volumeName: pvc-800d7ecc-2d5f-4ef4-af83-1cd94c766d37 status: accessModes: - ReadWriteOnce capacity: storage: 5Gi phase: Bound Creating additonal Volumes The Operator allows you to modify your cluster with additonal Volumes.\nspec: ... additionalVolumes: - name: empty mountPath: /opt/empty targetContainers: - all volumeSource: emptyDir: {} This example will create an emptyDir and mount it to all Containers inside the Database-Pod.\nSpecific Settings for aws gp3 Storage For the gp3 Storage aws you can define more informations\nvolume: size: 1Gi storageClass: gp3 iops: 1000 # for EBS gp3 throughput: 250 # in MB/s for EBS gp3 The defined IOPS and Throughput will include in the PersistentVolumeClaim and send to the storage-Provisioner. Please keep in Mind, that on aws there is a CoolDown-Time as a limitation defined. For new Changes you need to wait 6 hours. Please also ensure to check the default and allowed values for IOPS and Throughput AWS docs.\nTo ensure that the settings are updates properly please define the Operator-Configuration \u0026lsquo;storage_resize_mode\u0026rsquo; from default to \u0026lsquo;mixed\u0026rsquo;\n","description":"To set up a cluster, the implementation is based on a description, as with the other Kubernetes deplyoments. To do this, the operator uses a document of type postgresql.\nYou can also find the basic minimum specifications for a single-node cluster in our tutorial project on Github\nminimal Single-Node Cluster apiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: dockerImage: \u0026#34;docker.io/cybertecpostgresql/cybertec-pg-container:postgres-16.4-1\u0026#34; numberOfInstances: 1 postgresql: version: \u0026#34;16\u0026#34; resources: limits: cpu: 500m memory: 500Mi requests: cpu: 500m memory: 500Mi volume: size: 5Gi Based on this Manifest the Operator will deploy a single-Node-Cluster based on the defined dockerImage and start the included Postgres-16-Server. Also created is a volume based on your default-storage Class. The Ressource-Definiton means, that we reserve a half cpu and a half GB Memory for this Cluster with the same Definition as limit.\n"},{"id":25,"href":"/CYBERTEC-pg-operator/resources/","title":"Apply Ressources","parent":"CPO (CYBERTEC-PG-Operator)","content":"Kubernetes workloads are often deployed without a direct resource definition. This means that, apart from the limits specified by the administrators, the workloads can use the required resources of the worker node very dynamically.\nThe cluster manifest is used to define the Postgres pod resources via the typical resources objects.\nThere are basically two different definitions:\nrequests: Basic requirement and guaranteed by the worker node limits: maximum availability, allocation is increased dynamically if the worker node can provide the resources. For the planning of the cluster, a proper definition should be carried out in terms of the required hardware, which is then defined as requests. These resources are thus guaranteed to the cluster and are taken into account when deploying the pod. Accordingly, a pod can only be deployed on a worker if it can provide these resources. Any limits beyond this are not taken into account when deploying.\nThe unit of measurement should be taken into account when planning the necessary CPUs: cpu specifications are based on millicores\n1 cpu corresponds to 1 core 1 core corresponds to 1000 millicores (m) 1/2 core corresponds to 500 m resources: limits: cpu: 500m memory: 1Gi requests: cpu: 1000m memory: 1Gi This example corresponds to a guaranteed availability of half a core and 1 Gibibyte. However, if necessary and available, the container can use up to one core. The allocation takes place dynamically and for the required time.\nPods can be categorised into three Quality of Services (QoS) based on the defined information on the resources.\nBest-Effort: The containers of a pod have no resource information Burstable: A container of the pod has a memory or CPU requests or limits. Guaranteed: Each container of a pod has both cpu and memory requests and limits. In addition, the details of the respective limits correspond to the requests details If you would like more information and explanations, you can take a look at the Kubernetes documentation on QoS.\nWe generally recommend using the Guaranteed Status for PostgreSQL workloads. However, many users very successfully use the deviation of the CPU limit by factors such as 2. For example:\nresources: limits: cpu: 1000m memory: 1Gi requests: cpu: 2000m memory: 1Gi This is intended to create the possibility of additional CPU resources for sudden load peaks.\nHINT: The use of burstable definitions does not release you from a correct resource calculation, as limits resources are not guaranteed and therefore an undersupply can occur if the requests are not properly defined.\n","description":"Kubernetes workloads are often deployed without a direct resource definition. This means that, apart from the limits specified by the administrators, the workloads can use the required resources of the worker node very dynamically.\nThe cluster manifest is used to define the Postgres pod resources via the typical resources objects.\nThere are basically two different definitions:\nrequests: Basic requirement and guaranteed by the worker node limits: maximum availability, allocation is increased dynamically if the worker node can provide the resources. For the planning of the cluster, a proper definition should be carried out in terms of the required hardware, which is then defined as requests. These resources are thus guaranteed to the cluster and are taken into account when deploying the pod. Accordingly, a pod can only be deployed on a worker if it can provide these resources. Any limits beyond this are not taken into account when deploying.\n"},{"id":26,"href":"/CYBERTEC-pg-operator/storage/","title":"Storage","parent":"CPO (CYBERTEC-PG-Operator)","content":"Storage is crucial for the performance of a database and is therefore a central element. As with systems based on bare metal or virtual machines, the same requirements apply to Kubernetes workloads, such as constant availability, good performance, consistency and durability.\nA basic distinction is made between local storage, which is directly connected to the worker node, and network storage, which is mounted on the worker node and thus made available to the pod.\nIn probably the vast majority of Kubernetes systems, network storage is used, for example from systems from hyperscalers or other cloud providers or own systems such as CEPH.\nWith network storage in particular, attention must be paid to performance in terms of throughput (speed and guaranteed IOPS) and, above all, latency. It is also important to ensure that the different volumes do not compete with each other in terms of load.\nPAY ATTENTION: Before using a CPO cluster, make sure that the storage is suitable for the intended use and provides the necessary performance. In addition, check the storage with benchmarks before use. We recommend the use of pgbench for this purpose.\nDefine Storage-Volume The storage is defined via the volume object and enables the size and storage class for the storage to be defined, among other things.\nspec: volume: size: 5Gi storageClass: default-provisioner ... The volume is currently used for both PG and WAL data. In future, there will be an optional option to create a separate WAL volume. Please check our roadmap\nPAY ATTENTION: Please ensure, that the storageClass exists and is usable. If a Volume cannot provide the Volume will stand in the pending-State as like the Database-Pod.\nThe volume is currently used for both PG and WAL data. In future, there will be an optional option to create a separate WAL volume.\nExpanding Volume HINT: Kubernetes is able to forward requests to expand the storage to the storage system and enable the expand without the need to restart the container. However, this also requires the associated storage system and the driver used to support this. This information can be found in the storage class under the object: allowVolumeExpansion. A distinction must also be made between online and offline expand. The latter requires a restart of the pod. To do this, the pod must be deleted manually.\nTo Expand the Volume, the value of the object volume.size must be increased\nspec: volume: size: 10Gi storageClass: default-provisioner ... This will trigger the expand of your Cluster-Volumes. It will need some time and you can check the current state inside the pvc.\nkubectl get pvc pgdata-cluster-1-0 -o yaml ------------------------------------------------------- spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: crc-csi-hostpath-provisioner volumeMode: Filesystem volumeName: pvc-800d7ecc-2d5f-4ef4-af83-1cd94c766d37 status: accessModes: - ReadWriteOnce capacity: storage: 5Gi phase: Bound Creating additonal Volumes The Operator allows you to modify your cluster with additonal Volumes.\nspec: ... additionalVolumes: - name: empty mountPath: /opt/empty targetContainers: - all volumeSource: emptyDir: {} This example will create an emptyDir and mount it to all Containers inside the Database-Pod.\nSpecific Settings for aws gp3 Storage For the gp3 Storage aws you can define more informations\nvolume: size: 1Gi storageClass: gp3 iops: 1000 # for EBS gp3 throughput: 250 # in MB/s for EBS gp3 The defined IOPS and Throughput will include in the PersistentVolumeClaim and send to the storage-Provisioner. Please keep in Mind, that on aws there is a CoolDown-Time as a limitation defined. For new Changes you need to wait 6 hours. Please also ensure to check the default and allowed values for IOPS and Throughput AWS docs.\nTo ensure that the settings are updates properly please define the Operator-Configuration \u0026lsquo;storage_resize_mode\u0026rsquo; from default to \u0026lsquo;mixed\u0026rsquo;\n","description":"Storage is crucial for the performance of a database and is therefore a central element. As with systems based on bare metal or virtual machines, the same requirements apply to Kubernetes workloads, such as constant availability, good performance, consistency and durability.\nA basic distinction is made between local storage, which is directly connected to the worker node, and network storage, which is mounted on the worker node and thus made available to the pod.\n"},{"id":27,"href":"/CYBERTEC-pg-operator/db_users/","title":"Databases \u0026 Users","parent":"CPO (CYBERTEC-PG-Operator)","content":"CPO not only supports you in deploying your cluster, it also supports you in setting it up in terms of the database and users. CPO offers you three different options for this:\nCreate roles Create databases preapared databases Create Roles The creation of users is based on the definition of the user name and the definition of the required rights for this user. Available rights are\nsuperuser inherit login nologin createrole createdb replication bypassrls Unless explicitly defined via NOLOGIN, a created user automatically receives the LOGIN permission.\nspec: users: db_owner: - login - createdb appl_user: - login For each user created, CPO automatically creates a secret with username and password in the namespace of the cluster, which follows the following naming convention: [USERNAME].[CLUSTERNAME].credentials.postgresql.cpo.opensource.cybertec.at\nIf the secrets for an application are to be stored in a different namespace, for example, it is necessary to define the setting enable_cross_namespace_secret as true in the operator configuration. You can find more information about the operator configuration here.\nThe namespace must then be written before the user name.\nspec: users: db_owner: - login - createdb app_namespace.appl_user: - login Create Databases Databases are basically created in a very similar way to users. The definition is based on the database name and the database owner.\nspec: users: db_owner: - login - createdb app_namespace.appl_user: - login databases; app_db: app_namespace.appl_user HINT: Be aware that the user name must be defined for the database owner in the same way as it is done in the users object.\nPrepared Databases The preparedDatabases object is available for a much more extensive setup of databases and users. In addition to the creation of databases and users, this also enables the creation of schemas and extensions. A more detailed rights management is also available.\nDatabases and Schema Creating the preparedDatabases object already creates a database whose name is based on the cluster name. preparedDatabases: {}\nHINT: For the database name, - is replaced with _ in the cluster name\nTo create your own database names and elements such as schemas and extensions within the database, an object must be created within preparedDatabases for each database.\nspec: preparedDatabases: appl_db: extensions: dblink: public schemas: data: {} This example creates a database with the name appl_db and creates a schema with the name data in it, as well as creating the dblink extension in the schema public.\nManagement of users and Permissions For rights management, we distinguish between NOLOGIN roles and LOGIN roles. Users have login rights and inherit the other rights from the NOLOGIN role.\nNoLogin roles (defaultRoles) The roles are created if defaultroles is not explicitly set to false.\nspec: preparedDatabases: appl_db: extensions: dblink: public schemas: data: {} This creates roles for the schema owner, writer and reader\nLogin roles (defaultUsers) The roles described in the previous paragraph can be assigned to LOGIN roles via the users section in the manifest. Optionally, the Postgres operator can also create standard LOGIN roles for the database and each individual schema. These roles are given the suffix _user and inherit all rights from their NOLOGIN counterparts. Therefore, you cannot set defaultRoles to false and activate defaultUsers at the same time.\nspec: preparedDatabases: appl_db: defaultUsers: true extensions: dblink: public schemas: data: {} history: defaultRoles: true defaultUsers: false This example creates the following users and inheritances\nRole name Attributes inherits from appl_db_owner Cannot login appl_db_reader,appl_db_owner,appl_data_owner,\u0026hellip; appl_db_owner_user appl_db_owner appl_db_reader Cannot login appl_db_reader_user appl_db_reader appl_db_writer Cannot login appl_db_reader appl_db_writer_user appl_db_writer appl_db_data_owner Cannot login appl_db_data_reader,appl_db_data_writer appl_db_data_reader Cannot login appl_db_data_writer Cannot login appl_db_data_reader appl_db_history_owner Cannot login appl_db_history_reader,appl_db_history_writer appl_db_history_reader Cannot login appl_db_history_writer Cannot login appl_db_history_reader Default access permissions are also defined for LOGIN roles when databases and schemas are created. This means that they are not currently set if defaultUsers (or defaultRoles for schemas) are activated at a later time.\nUser Secrets For each user created by cpo with LOGIN permissions, the operator also creates a secret with username and password, as with the creation of roles via the users object.\n","description":"CPO not only supports you in deploying your cluster, it also supports you in setting it up in terms of the database and users. CPO offers you three different options for this:\nCreate roles Create databases preapared databases Create Roles The creation of users is based on the definition of the user name and the definition of the required rights for this user. Available rights are\n"},{"id":28,"href":"/CYBERTEC-pg-operator/customize_cluster/","title":"Customize Cluster","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":29,"href":"/CYBERTEC-pg-operator/ha_cluster/","title":"High Availability","parent":"CPO (CYBERTEC-PG-Operator)","content":"High availability (HA) is a critical aspect of running database systems, especially in mission-critical applications where downtime is unacceptable. This section explains why high availability is important for PostgreSQL and how Patroni acts as a solution to ensure HA. Why High Availability (HA) for PostgreSQL?\nTo minimise downtime: In modern, data-driven applications, downtime can cause significant financial and reputational losses. High availability ensures that the database remains available even in the event of hardware failures or network problems. Data integrity and security: A database failure can lead to data loss or data inconsistencies. High-availability solutions protect against such scenarios through continuous data replication and automatic failover. Scalability and load balancing: HA setups make it possible to distribute the load across multiple nodes, resulting in better performance and faster response times. This is particularly important in environments with high data traffic. Ease of maintenance: By setting up high availability, database maintenance can be performed without interrupting services. Nodes can be maintained incrementally while the database remains available. Patroni - the cluster manager In our PostgreSQL environment, we use Patroni in the PG containers by default. This has the advantage that even single-node instances basically function as Patroni clusters. This configuration offers several important advantages:\nEasy scalability: by using Patroni in all PG containers, scaling pods up and down is possible at any time. You can easily add additional pods as needed to improve performance or increase capacity, or remove pods to free up resources. This flexibility is particularly useful in dynamic environments where requirements can change quickly. Automated cluster management: Patroni automatically takes over the management of the cluster. When a new pod is added to an existing cluster, Patroni takes care of setting up the new node itself, including initialising and starting replication. This means you don\u0026rsquo;t have to perform any manual steps to configure or manage new nodes - Patroni does it all for you automatically. Seamless integration: As Patroni is active in every PG container by default, you don\u0026rsquo;t have to worry about compatibility or manual configuration. This makes deployment and maintenance much easier, as all the necessary components are already preconfigured. Optimisation of resources: Even with a minimal setup (single-node instance), you benefit from the advantages of a Patroni cluster, including the possibility of easy expansion and automatic failover in the event of a failure. This ensures optimal resource utilisation and minimises downtime. Upgrade the cluster to high availability The necessary changes to a high-availability cluster are very limited. Only the number of desired instances needs to be increased.\napiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: dockerImage: \u0026#34;docker.io/cybertecpostgresql/cybertec-pg-container:postgres-16.4-1\u0026#34; numberOfInstances: 2 postgresql: version: \u0026#34;16\u0026#34; resources: limits: cpu: 500m memory: 500Mi requests: cpu: 500m memory: 500Mi volume: size: 5Gi You can either create a new cluster with the document or update an existing cluster with it. This makes it possible to scale the cluster up and down during operation.\nThe example above will create a HA-Cluster based on two Nodes.\nkubectl get pods ----------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cluster-1-0 | 1/1 | Running | 0 | 3d cluster-1-1 | 1/1 | Running | 0 | 31s ","description":"High availability (HA) is a critical aspect of running database systems, especially in mission-critical applications where downtime is unacceptable. This section explains why high availability is important for PostgreSQL and how Patroni acts as a solution to ensure HA. Why High Availability (HA) for PostgreSQL?\nTo minimise downtime: In modern, data-driven applications, downtime can cause significant financial and reputational losses. High availability ensures that the database remains available even in the event of hardware failures or network problems. Data integrity and security: A database failure can lead to data loss or data inconsistencies. High-availability solutions protect against such scenarios through continuous data replication and automatic failover. Scalability and load balancing: HA setups make it possible to distribute the load across multiple nodes, resulting in better performance and faster response times. This is particularly important in environments with high data traffic. Ease of maintenance: By setting up high availability, database maintenance can be performed without interrupting services. Nodes can be maintained incrementally while the database remains available. Patroni - the cluster manager In our PostgreSQL environment, we use Patroni in the PG containers by default. This has the advantage that even single-node instances basically function as Patroni clusters. This configuration offers several important advantages:\n"},{"id":30,"href":"/CYBERTEC-pg-operator/config_cluster/","title":"PostgreSQL Configuration","parent":"CPO (CYBERTEC-PG-Operator)","content":"Users who are already used to working with PostgreSQL from Baremetal or VMs are already familiar with the need for various files to configure PostgreSQL. These include\npostgresql.conf pg_hba.conf \u0026hellip; Although these files are available in the container, direct modification is not planned. As part of the declarative mode of operation of the operator, these files are defined via the operator. The modifying intervention within the container also represents a contradiction to the immutability of the container.\nFor these reasons, the operator provides a way to make adjustments to the various files, from PostgreSQL to Patroni.\nWe differentiate between two main objects in the cluster manifest:\npostgresql with the child objects version and parameters patroni with objects for the pg_hab, slots and much more postgresql The postgresql object consists of the following elements:\nversion - allows you to select the major version of PostgreSQL used. parameters- enables the postgresql.conf to be changed spec: postgresql: parameters: shared_preload_libraries: \u0026#39;pg_stat_statements,pgnodemx, timescaledb\u0026#39; shared_buffers: \u0026#39;512MB\u0026#39; version: \u0026#39;16\u0026#39; Any known PostgreSQL parameter from postgresql.conf can be entered here and will be delivered by the operator to all nodes of the cluster accordingly.\nYou can find more information about the parameters in the PostgreSQL documentation\npatroni The patroni object contains numerous options for customising the patroni-setu, and the pg_hba.conf is also configured here. A complete list of all available elements can be found here.\nThe most important elements include\npg_hba - pg_hba.conf slots synchronous_mode - enables synchronous mode in the cluster. The default is set to false maximum_lag_on_failover - Specifies the maximum lag so that the pod is still considered healthy in the event of a failover. failsafe_mode Allows you to cancel the downgrading of the leader if all cluster members can be reached via the Patroni Rest Api. You can find more information on this in the Patroni documentation pg_hba The pg_hba.conf contains all defined authentication rules for PostgreSQL.\nWhen customising this configuration, it is important that the entire version of pg_hba is written to the manifest. The current configuration can be read out in the database using table pg_hba_file_rules ;.\nFurther information can be found in the PostgreSQL documentation\nslots When using user-defined slots, for example for the use of CDC using Debezium, there are problems when interacting with Patroni, as the slot and its current status are not automatically synchronised to the replicas.\nIn the event of a failover, the client cannot start replication as both the entire slot and the information about the data that has already been synchronised are missing.\nTo resolve this problem, slots must be defined in the cluster manifest rather than in PostgreSQL.\nspec: patroni: slots: cdc-example: database: app_db plugin: pgoutput type: logical This example creates a logical replication slot with the name cdc-example within the app_db database and uses the pgoutput plugin for the slot.\nATTENTION: Slots are only synchronised from the leader/standby leader to the replicas. This means that using the slots read-only on the replicas will cause a problem in the event of a failover.\n","description":"Users who are already used to working with PostgreSQL from Baremetal or VMs are already familiar with the need for various files to configure PostgreSQL. These include\npostgresql.conf pg_hba.conf \u0026hellip; Although these files are available in the container, direct modification is not planned. As part of the declarative mode of operation of the operator, these files are defined via the operator. The modifying intervention within the container also represents a contradiction to the immutability of the container.\n"},{"id":31,"href":"/CYBERTEC-pg-operator/backup/","title":"Backup","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":32,"href":"/CYBERTEC-pg-operator/restore/","title":"Restore","parent":"CPO (CYBERTEC-PG-Operator)","content":"Restore or recovery is the process of starting a PostgreSQL instance or a cluster based on a defined and existing backup. This can be just a Backup or a combination of a Backup and additional WAL files. The difference is that a Backup represents a fixed point in time, whereas the combination with WAL enables a point-in-time recovery(PITR).\nYou can find more information about backups here\nRescue my cluster CPO enables the restore based on an existing backup using pgBackRest. To do this, it needs the relevant information about the point in time or snapBackupshot to which it should restore and where the data for this comes from. As we have already provided the operator with all the information relating to the storage of backups in the previous chapter, it only needs the following information:\nid: Control variable, must be increased for each restore process type: What type of restore is required repo: Which repo the data should come from set: Specific Backup to restore - Check backup to see how to get the identifier HINT: To ensure that the operator does not repeat an already done restore, the defined object id in the restore section is saved by the operator, so the value of this id must be changed for a new restore.\nDetails for a Backup restore With this information, we define a fixed Backup from repo1 and that pgBackRest should stop at the end of the Backup\nrestore: id: \u0026#39;1\u0026#39; options: type: \u0026#39;immediate\u0026#39; set: \u0026#39;20240515-164100F\u0026#39; repo: \u0026#39;repo1\u0026#39; Without the specification --type=immediate, pgBackRest would then consume the entire WAL that is available and thus restore the last available consistent data point. Details for a point-in-time recoery (PITR) We use this information to define a point-in-time recovery (PITR) and define the end point using a timestamp and the start point using a Backup specification. The latter is optional. Without this information, pgBackRest would automatically start at the last previous full Backup.\nrestore: id: \u0026#39;1\u0026#39; options: type: \u0026#39;time\u0026#39; set: \u0026#39;20240515-164100F\u0026#39; target: \u0026#39;2024-05-16 07:46:05.506817+00\u0026#39; repo: \u0026#39;1\u0026#39; --type=time indicates that it is a point-in-time recovery (PITR). Example in a cluster manifest apiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-5 namespace: cpo spec: backup: pgbackrest: configuration: secret: cluster-1-pvc-credentials global: repo1-retention-full: \u0026#39;7\u0026#39; repo1-retention-full-type: count image: \u0026#39;docker.io/cybertecpostgresql/cybertec-pg-container:pgbackrest-16.4-1\u0026#39; repos: - name: repo1 schedule: full: 30 2 * * * storage: pvc volume: size: 1Gi restore: id: \u0026#39;1\u0026#39; options: type: \u0026#39;time\u0026#39; set: \u0026#39;20240515-164100F\u0026#39; target: \u0026#39;2024-05-16 07:46:05.506817+00\u0026#39; An example of this can also be found in our tutorials. For a point-in-time recovery (PITR) you can find it here.\nIncorrect information for the Backup or the timestamp can result in pgBackRest not being able to complete the restore successfully. In the event of an error, the information must be corrected and another restore must be started. ","description":"Restore or recovery is the process of starting a PostgreSQL instance or a cluster based on a defined and existing backup. This can be just a Backup or a combination of a Backup and additional WAL files. The difference is that a Backup represents a fixed point in time, whereas the combination with WAL enables a point-in-time recovery(PITR).\nYou can find more information about backups here\nRescue my cluster CPO enables the restore based on an existing backup using pgBackRest. To do this, it needs the relevant information about the point in time or snapBackupshot to which it should restore and where the data for this comes from. As we have already provided the operator with all the information relating to the storage of backups in the previous chapter, it only needs the following information:\n"},{"id":33,"href":"/CYBERTEC-pg-operator/tls/","title":"TLS/SSL connections","parent":"CPO (CYBERTEC-PG-Operator)","content":"Each cluster created is automatically equipped with a self-generated TLS certificate and is preconfigured for the use of TLS/SSL. However, this certificate is not based on a Certificate Authority (CA) that is known to the clients. This means that although communication between the client and server is encrypted, the certificate cannot be verified by the client.\nThe following chapter deals with the creation of custom certificates and the steps required to integrate these certificates into the PostgreSQL cluster. In the example, a custom CA is created, on the basis of which the certificates are then generated and signed by this CA. This step can be skipped if certificates have already been obtained from another trusted organisation.\nCreate a custom CA and Certificates Precondition: This chapter requires openssl Create the CA The first step is to create a custom CA. An organisation name is required for this. You can also add further details about the country, district and location. The CA serves as the central authority that signs the certificates and thus guarantees the correctness of the certificate. In order to successfully complete the verification of a certificate, the CA\u0026rsquo;s certificate must be stored on the client system.\nORGANIZATION=MyCustomOrganization CA=$ORGANIZATION-RootCA mkdir $CA cd $CA # Creating the CA-Key openssl genpkey -algorithm EC -out $CA.key -pkeyopt ec_paramgen_curve:secp384r1 -pkeyopt ec_param_enc:named_curve -aes256 # Creating the CA-Certificate openssl req -x509 -new -nodes -key $CA.key -sha512 -days 1826 -out $CA.crt -subj \u0026#34;/CN=${ORGANIZATION} Root-CA/C=AT/ST=Lower Austria/L=Woellersdorf/O=${ORGANIZATION}\u0026#34; Create a custom Certificate The server needs a certificate signed by a CA and a private key so that it can claim to be trustworthy.\nIt is important that the CA certificate is stored as trustworthy with the client. Otherwise, no certificate check is possible. CN=cluster-1 DNS2=\u0026#34;${CN}-repl\u0026#34; DNS3=\u0026#34;${CN}-pooler\u0026#34; DNS4=\u0026#34;${CN}-pooler-repl\u0026#34; # Creating the private Key openssl genpkey -algorithm EC -out $CN.key -pkeyopt ec_paramgen_curve:secp384r1 -pkeyopt ec_param_enc:named_curve # Creating Certificate Signing Request (CSR)) openssl req -new -key $CN.key -out $CN.csr \\ -subj \u0026#34;/C=AT/ST=Lower Austria/L=Woellersdorf/O=${ORGANIZATION}/OU=OrgUnit/CN=${CN}\u0026#34; \\ -addext \u0026#34;subjectAltName=DNS:${CN},DNS:${DNS2},DNS:${DNS3},DNS:${DNS4}\u0026#34; # Sign CSR with the CA openssl x509 -req -in $CN.csr -CA $CA.crt -CAkey $CA.key -CAcreateserial -out $CN.crt -days 365 \\ -extfile \u0026lt;(echo -e \u0026#34;[ v3_req ]\\nsubjectAltName=DNS:${CN},DNS:${DNS2},DNS:${DNS3},DNS:${DNS4}\u0026#34;) -extensions v3_req Add Certicate to the Cluster For adding the Certificate to your cluster a secret on kubernetes is needed. There are two different options here. For the first option, a secret is created that contains all the necessary information. I.e.\nServer certificate Private server key CA certificate In the second variant, the CA certificate is separated and written in a separate secret. The advantage of this is that the CA only needs to be saved once and changed in the event of an update. First Option: Using one secret for all three objects kubectl create secret generic cluster-1-tls \\ --from-file=tls.crt=$CN.crt \\ --from-file=tls.key=$CN.key \\ --from-file=ca.crt=$CA.crt Finally, the definition is made in the cluster manifest so that the operator adapts the cluster.\napiVersion: \u0026#34;cpo.opensource.cybertec.at/v1\u0026#34; kind: postgresql ... metadata: name: cluster-1 spec: tls: secretName: \u0026#34;cluster-1-tls\u0026#34; caFile: \u0026#34;ca.crt\u0026#34; Second Option: Using a separat Secret for the CA kubectl create secret generic cpo-root-ca --from-file=ca.crt=ca.crt kubectl create secret generic cluster-1-tls \\ --from-file=tls.crt=$CN.crt \\ --from-file=tls.key=$CN.key \\ Finally, the definition is made in the cluster manifest so that the operator adapts the cluster.\napiVersion: \u0026#34;cpo.opensource.cybertec.at/v1\u0026#34; kind: postgresql metadata: name: cluster-1 spec: tls: secretName: \u0026#34;cluster-1-tls\u0026#34; caSecretName: \u0026#34;cpo-root-ca\u0026#34; caFile: \u0026#34;ca.crt\u0026#34; A regular check of the mounted certificates takes place automatically within the container. This check takes place every 5 minutes. If the certificates have been updated, the certificates are loaded automatically.\nIn addition to generating the certificates independently, cert-manager can also be used for this purpose. ","description":"Each cluster created is automatically equipped with a self-generated TLS certificate and is preconfigured for the use of TLS/SSL. However, this certificate is not based on a Certificate Authority (CA) that is known to the clients. This means that although communication between the client and server is encrypted, the certificate cannot be verified by the client.\nThe following chapter deals with the creation of custom certificates and the steps required to integrate these certificates into the PostgreSQL cluster. In the example, a custom CA is created, on the basis of which the certificates are then generated and signed by this CA. This step can be skipped if certificates have already been obtained from another trusted organisation.\n"},{"id":34,"href":"/CYBERTEC-pg-operator/connection_pooler/","title":"connection pooler","parent":"CPO (CYBERTEC-PG-Operator)","content":"A connection pooler is a tool that acts as a proxy between the application and the database and enables the performance of the application to be improved and the load on the database to be reduced. The reason for this lies in the connection handling of PostgreSQL.\nHow PostgreSQL handles connection PostgreSQL use a new Process for every database-connection created by the postmaster. This process is handling the connection. On the positive side, this enables a stable connection and isolation, but it is not particularly efficient for short-lived connections due to the effort required to create them.\nHow Connection Pooling solves this problem With connection pooling, the application connects to the pooler, which in turn maintains a number of connections to the PostgreSQL database. This makes it possible to use the connections from the pooler to the database for a long time instead of short-lived connections and to recycle them accordingly.\nIn addition to utilising long-term connections, a ConnectionPooler also makes it possible to reduce the number of connections required to the database. For example, if you have 3 application nodes, each of which maintains 100 connections to the database at the same time, that would be 300 connections in total. The application usually does not even begin to utilise this number of connections. With the pgBouncer, this can be optimised so that the applications open the 300 connections to the pgBouncer, but the pgBouncer only generates 100 connections to PostgreSQL, for example, thus reducing the load by 2/3.\nHINT: It is important to correctly configure the bouncer and thus the connections to be created between pgBouncer and PostgreSQL so that enough connections are available for the workload.\nHow does this work with CPO CPO relies on pgBouncer, a popular and above all lightweight open source tool. pgBouncer manages individual user-database connections for each user used, which can be used immediately for incoming client connections.\nHow do I create a pooler for a cluster? connection_pooler.number_of_instances - How many instances of connection pooler to create. Default is 2 which is also the required minimum.\nconnection_pooler.schema - Database schema to create for credentials lookup function to be used by the connection pooler. Is is created in every database of the Postgres cluster. You can also choose an existing schema. Default schema is pooler.\nconnection_pooler.user - User to create for connection pooler to be able to connect to a database. You can also choose an existing role, but make sure it has the LOGIN privilege. Default role is pooler.\nconnection_pooler.image - Docker image to use for connection pooler deployment. Default: registry.opensource.zalan.do/acid/pgbouncer\nconnection_poole.max_db_connections - How many connections the pooler can max hold. This value is divided among the pooler pods. Default is 60 which will make up 30 connections per pod for the default setup with two instances.\nconnection_pooler.mode - Defines pooler mode. Available Value: session, transaction or statement. Default is transaction.\nconnection_pooler.resources - Hardware definition for the pooler pods\nenableConnectionPooler - Defines whether poolers for read/write access should be created based on the spec.connectionPooler definition.\nenableReplicaConnectionPooler- Defines whether poolers for read-only access should be created based on the spec.connectionPooler definition.\nspec: connectionPooler: mode: transaction numberOfInstances: 2 resources: limits: cpu: \u0026#39;1\u0026#39; memory: 100Mi requests: cpu: 500m memory: 100Mi schema: pooler user: pooler enableConnectionPooler: true enableReplicaConnectionPooler: true ","description":"A connection pooler is a tool that acts as a proxy between the application and the database and enables the performance of the application to be improved and the load on the database to be reduced. The reason for this lies in the connection handling of PostgreSQL.\nHow PostgreSQL handles connection PostgreSQL use a new Process for every database-connection created by the postmaster. This process is handling the connection. On the positive side, this enables a stable connection and isolation, but it is not particularly efficient for short-lived connections due to the effort required to create them.\n"},{"id":35,"href":"/CYBERTEC-pg-operator/extensions/pg13/","title":"PostgreSQL 13","parent":"Extensions","content":" The extensions listed are included in the standard images. This list refers to PostgreSQL 13. Name Default Version Comment adminpack 2.1 Administrative functions for PostgreSQL amcheck 1.2 Functions for verifying relation integrity autoinc 1.0 Functions for autoincrementing fields bloom 1.0 Bloom access method - signature file based index btree_gin 1.3 Support for indexing common datatypes in GIN btree_gist 1.5 Support for indexing common datatypes in GiST citext 1.6 Data type for case-insensitive character strings credcheck 3.0.0 credcheck - PostgreSQL plain text credential checker cube 1.4 Data type for multidimensional cubes dblink 1.2 Connect to other PostgreSQL databases from within a database dict_int 1.0 Text search dictionary template for integers dict_xsyn 1.0 Text search dictionary template for extended synonym processing earthdistance 1.1 Calculate great-circle distances on the surface of the Earth file_fdw 1.0 Foreign-data wrapper for flat file access fuzzystrmatch 1.1 Determine similarities and distance between strings hstore 1.7 Data type for storing sets of (key, value) pairs hstore_plperl 1.0 Transform between hstore and plperl hstore_plperlu 1.0 Transform between hstore and plperlu hstore_plpython3u 1.0 Transform between hstore and plpython3u insert_username 1.0 Functions for tracking who changed a table intagg 1.1 Integer aggregator and enumerator (obsolete) intarray 1.3 Functions, operators, and index support for 1-D arrays of integers isn 1.2 Data types for international product numbering standards jsonb_plperl 1.0 Transform between jsonb and plperl jsonb_plperlu 1.0 Transform between jsonb and plperlu jsonb_plpython3u 1.0 Transform between jsonb and plpython3u lo 1.1 Large Object maintenance ltree 1.2 Data type for hierarchical tree-like structures ltree_plpython3u 1.0 Transform between ltree and plpython3u moddatetime 1.0 Functions for tracking last modification time pageinspect 1.8 Inspect the contents of database pages at a low level pg_buffercache 1.3 Examine the shared buffer cache pg_cron 1.6 Job scheduler for PostgreSQL pg_freespacemap 1.2 Examine the free space map (FSM) pg_permissions 1.3 View object permissions and compare them with the desired state pg_prewarm 1.2 Prewarm relation data pg_proctab Placeholder - see pg_proctab\u0026ndash;0.0.10-compat.control pg_stat_statements 1.8 Track planning and execution statistics of all SQL statements executed pg_trgm 1.5 Text similarity measurement and index searching based on trigrams pg_visibility 1.2 Examine the visibility map (VM) and page-level visibility info pgaudit 1.5.3 Provides auditing functionality pgauditlogtofile 1.6 pgAudit addon to redirect audit entries to an independent file pgcrypto 1.3 Cryptographic functions pgnodemx 1.7 SQL functions that allow capture of node OS metrics from PostgreSQL pgrowlocks 1.2 Show row-level locking information pgstattuple 1.5 Show tuple-level statistics plpgsql 1.0 PL/pgSQL procedural language plpython3u 1.0 PL/Python3U untrusted procedural language pltcl 1.0 PL/Tcl procedural language pltclu 1.0 PL/TclU untrusted procedural language postgres_fdw 1.0 Foreign-data wrapper for remote PostgreSQL servers refint 1.0 Functions for implementing referential integrity (obsolete) seg 1.3 Data type for representing line segments or floating-point intervals set_user 4.1.0 Similar to SET ROLE but with added logging sslinfo 1.2 Information about SSL certificates tablefunc 1.0 Functions that manipulate whole tables, including crosstab tcn 1.0 Triggered change notifications timescaledb 2.15.3 Enables scalable inserts and complex queries for time-series data (Apache 2 Edition) tsm_system_rows 1.0 TABLESAMPLE method which accepts number of rows as a limit tsm_system_time 1.0 TABLESAMPLE method which accepts time in milliseconds as a limit unaccent 1.1 Text search dictionary that removes accents uuid-ossp 1.1 Generate universally unique identifiers (UUIDs) xml2 1.1 XPath querying and XSLT The following extensions are additionally included in the Postgis images. Name Default Version Comment address_standardizer 3.4.4 Used to parse an address into constituent elements. Generally used to support geocoding address normalization step. address_standardizer_data_us 3.4.4 Address Standardizer US dataset example postgis 3.4.4 PostGIS geometry and geography spatial types and functions postgis_raster 3.4.4 PostGIS raster types and functions postgis_sfcgal 3.4.4 PostGIS SFCGAL functions postgis_tiger_geocoder 3.4.4 PostGIS tiger geocoder and reverse geocoder postgis_topology 3.4.4 PostGIS topology spatial types and functions ","description":" The extensions listed are included in the standard images. This list refers to PostgreSQL 13. Name Default Version Comment adminpack 2.1 Administrative functions for PostgreSQL amcheck 1.2 Functions for verifying relation integrity autoinc 1.0 Functions for autoincrementing fields bloom 1.0 Bloom access method - signature file based index btree_gin 1.3 Support for indexing common datatypes in GIN btree_gist 1.5 Support for indexing common datatypes in GiST citext 1.6 Data type for case-insensitive character strings credcheck 3.0.0 credcheck - PostgreSQL plain text credential checker cube 1.4 Data type for multidimensional cubes dblink 1.2 Connect to other PostgreSQL databases from within a database dict_int 1.0 Text search dictionary template for integers dict_xsyn 1.0 Text search dictionary template for extended synonym processing earthdistance 1.1 Calculate great-circle distances on the surface of the Earth file_fdw 1.0 Foreign-data wrapper for flat file access fuzzystrmatch 1.1 Determine similarities and distance between strings hstore 1.7 Data type for storing sets of (key, value) pairs hstore_plperl 1.0 Transform between hstore and plperl hstore_plperlu 1.0 Transform between hstore and plperlu hstore_plpython3u 1.0 Transform between hstore and plpython3u insert_username 1.0 Functions for tracking who changed a table intagg 1.1 Integer aggregator and enumerator (obsolete) intarray 1.3 Functions, operators, and index support for 1-D arrays of integers isn 1.2 Data types for international product numbering standards jsonb_plperl 1.0 Transform between jsonb and plperl jsonb_plperlu 1.0 Transform between jsonb and plperlu jsonb_plpython3u 1.0 Transform between jsonb and plpython3u lo 1.1 Large Object maintenance ltree 1.2 Data type for hierarchical tree-like structures ltree_plpython3u 1.0 Transform between ltree and plpython3u moddatetime 1.0 Functions for tracking last modification time pageinspect 1.8 Inspect the contents of database pages at a low level pg_buffercache 1.3 Examine the shared buffer cache pg_cron 1.6 Job scheduler for PostgreSQL pg_freespacemap 1.2 Examine the free space map (FSM) pg_permissions 1.3 View object permissions and compare them with the desired state pg_prewarm 1.2 Prewarm relation data pg_proctab Placeholder - see pg_proctab\u0026ndash;0.0.10-compat.control pg_stat_statements 1.8 Track planning and execution statistics of all SQL statements executed pg_trgm 1.5 Text similarity measurement and index searching based on trigrams pg_visibility 1.2 Examine the visibility map (VM) and page-level visibility info pgaudit 1.5.3 Provides auditing functionality pgauditlogtofile 1.6 pgAudit addon to redirect audit entries to an independent file pgcrypto 1.3 Cryptographic functions pgnodemx 1.7 SQL functions that allow capture of node OS metrics from PostgreSQL pgrowlocks 1.2 Show row-level locking information pgstattuple 1.5 Show tuple-level statistics plpgsql 1.0 PL/pgSQL procedural language plpython3u 1.0 PL/Python3U untrusted procedural language pltcl 1.0 PL/Tcl procedural language pltclu 1.0 PL/TclU untrusted procedural language postgres_fdw 1.0 Foreign-data wrapper for remote PostgreSQL servers refint 1.0 Functions for implementing referential integrity (obsolete) seg 1.3 Data type for representing line segments or floating-point intervals set_user 4.1.0 Similar to SET ROLE but with added logging sslinfo 1.2 Information about SSL certificates tablefunc 1.0 Functions that manipulate whole tables, including crosstab tcn 1.0 Triggered change notifications timescaledb 2.15.3 Enables scalable inserts and complex queries for time-series data (Apache 2 Edition) tsm_system_rows 1.0 TABLESAMPLE method which accepts number of rows as a limit tsm_system_time 1.0 TABLESAMPLE method which accepts time in milliseconds as a limit unaccent 1.1 Text search dictionary that removes accents uuid-ossp 1.1 Generate universally unique identifiers (UUIDs) xml2 1.1 XPath querying and XSLT The following extensions are additionally included in the Postgis images. Name Default Version Comment address_standardizer 3.4.4 Used to parse an address into constituent elements. Generally used to support geocoding address normalization step. address_standardizer_data_us 3.4.4 Address Standardizer US dataset example postgis 3.4.4 PostGIS geometry and geography spatial types and functions postgis_raster 3.4.4 PostGIS raster types and functions postgis_sfcgal 3.4.4 PostGIS SFCGAL functions postgis_tiger_geocoder 3.4.4 PostGIS tiger geocoder and reverse geocoder postgis_topology 3.4.4 PostGIS topology spatial types and functions "},{"id":36,"href":"/CYBERTEC-pg-operator/extensions/pg14/","title":"PostgreSQL 14","parent":"Extensions","content":" The extensions listed are included in the standard images. This list refers to PostgreSQL 14. Name Default Version Comment adminpack 2.1 Administrative functions for PostgreSQL amcheck 1.3 Functions for verifying relation integrity autoinc 1.0 Functions for autoincrementing fields bloom 1.0 Bloom access method - signature file based index btree_gin 1.3 Support for indexing common datatypes in GIN btree_gist 1.6 Support for indexing common datatypes in GiST citext 1.6 Data type for case-insensitive character strings credcheck 3.0.0 credcheck - PostgreSQL plain text credential checker cube 1.5 Data type for multidimensional cubes dblink 1.2 Connect to other PostgreSQL databases from within a database dict_int 1.0 Text search dictionary template for integers dict_xsyn 1.0 Text search dictionary template for extended synonym processing earthdistance 1.1 Calculate great-circle distances on the surface of the Earth file_fdw 1.0 Foreign-data wrapper for flat file access fuzzystrmatch 1.1 Determine similarities and distance between strings hstore 1.8 Data type for storing sets of (key, value) pairs hstore_plperl 1.0 Transform between hstore and plperl hstore_plperlu 1.0 Transform between hstore and plperlu hstore_plpython3u 1.0 Transform between hstore and plpython3u insert_username 1.0 Functions for tracking who changed a table intagg 1.1 Integer aggregator and enumerator (obsolete) intarray 1.5 Functions, operators, and index support for 1-D arrays of integers isn 1.2 Data types for international product numbering standards jsonb_plperl 1.0 Transform between jsonb and plperl jsonb_plperlu 1.0 Transform between jsonb and plperlu jsonb_plpython3u 1.0 Transform between jsonb and plpython3u lo 1.1 Large Object maintenance ltree 1.2 Data type for hierarchical tree-like structures ltree_plpython3u 1.0 Transform between ltree and plpython3u moddatetime 1.0 Functions for tracking last modification time old_snapshot 1.0 Utilities in support of old_snapshot_threshold pageinspect 1.9 Inspect the contents of database pages at a low level pg_buffercache 1.3 Examine the shared buffer cache pg_cron 1.6 Job scheduler for PostgreSQL pg_freespacemap 1.2 Examine the free space map (FSM) pg_permissions 1.3 View object permissions and compare them with the desired state pg_prewarm 1.2 Prewarm relation data pg_proctab Placeholder - see pg_proctab\u0026ndash;0.0.10-compat.control pg_stat_statements 1.9 Track planning and execution statistics of all SQL statements executed pg_surgery 1.0 Extension to perform surgery on a damaged relation pg_trgm 1.6 Text similarity measurement and index searching based on trigrams pg_visibility 1.2 Examine the visibility map (VM) and page-level visibility info pgaudit 1.6.3 Provides auditing functionality pgauditlogtofile 1.6 pgAudit addon to redirect audit entries to an independent file pgcrypto 1.3 Cryptographic functions pgnodemx 1.7 SQL functions that allow capture of node OS metrics from PostgreSQL pgrowlocks 1.2 Show row-level locking information pgstattuple 1.5 Show tuple-level statistics plpgsql 1.0 PL/pgSQL procedural language plpython3u 1.0 PL/Python3U untrusted procedural language pltcl 1.0 PL/Tcl procedural language pltclu 1.0 PL/TclU untrusted procedural language postgres_fdw 1.1 Foreign-data wrapper for remote PostgreSQL servers refint 1.0 Functions for implementing referential integrity (obsolete) seg 1.4 Data type for representing line segments or floating-point intervals set_user 4.1.0 Similar to SET ROLE but with added logging sslinfo 1.2 Information about SSL certificates tablefunc 1.0 Functions that manipulate whole tables, including crosstab tcn 1.0 Triggered change notifications timescaledb 2.18.2 Enables scalable inserts and complex queries for time-series data (Apache 2 Edition) tsm_system_rows 1.0 TABLESAMPLE method which accepts number of rows as a limit tsm_system_time 1.0 TABLESAMPLE method which accepts time in milliseconds as a limit unaccent 1.1 Text search dictionary that removes accents uuid-ossp 1.1 Generate universally unique identifiers (UUIDs) xml2 1.1 XPath querying and XSLT The following extensions are additionally included in the Postgis images. Name Default Version Comment address_standardizer 3.4.4 Used to parse an address into constituent elements. Generally used to support geocoding address normalization step. address_standardizer_data_us 3.4.4 Address Standardizer US dataset example postgis 3.4.4 PostGIS geometry and geography spatial types and functions postgis_raster 3.4.4 PostGIS raster types and functions postgis_sfcgal 3.4.4 PostGIS SFCGAL functions postgis_tiger_geocoder 3.4.4 PostGIS tiger geocoder and reverse geocoder postgis_topology 3.4.4 PostGIS topology spatial types and functions ","description":" The extensions listed are included in the standard images. This list refers to PostgreSQL 14. Name Default Version Comment adminpack 2.1 Administrative functions for PostgreSQL amcheck 1.3 Functions for verifying relation integrity autoinc 1.0 Functions for autoincrementing fields bloom 1.0 Bloom access method - signature file based index btree_gin 1.3 Support for indexing common datatypes in GIN btree_gist 1.6 Support for indexing common datatypes in GiST citext 1.6 Data type for case-insensitive character strings credcheck 3.0.0 credcheck - PostgreSQL plain text credential checker cube 1.5 Data type for multidimensional cubes dblink 1.2 Connect to other PostgreSQL databases from within a database dict_int 1.0 Text search dictionary template for integers dict_xsyn 1.0 Text search dictionary template for extended synonym processing earthdistance 1.1 Calculate great-circle distances on the surface of the Earth file_fdw 1.0 Foreign-data wrapper for flat file access fuzzystrmatch 1.1 Determine similarities and distance between strings hstore 1.8 Data type for storing sets of (key, value) pairs hstore_plperl 1.0 Transform between hstore and plperl hstore_plperlu 1.0 Transform between hstore and plperlu hstore_plpython3u 1.0 Transform between hstore and plpython3u insert_username 1.0 Functions for tracking who changed a table intagg 1.1 Integer aggregator and enumerator (obsolete) intarray 1.5 Functions, operators, and index support for 1-D arrays of integers isn 1.2 Data types for international product numbering standards jsonb_plperl 1.0 Transform between jsonb and plperl jsonb_plperlu 1.0 Transform between jsonb and plperlu jsonb_plpython3u 1.0 Transform between jsonb and plpython3u lo 1.1 Large Object maintenance ltree 1.2 Data type for hierarchical tree-like structures ltree_plpython3u 1.0 Transform between ltree and plpython3u moddatetime 1.0 Functions for tracking last modification time old_snapshot 1.0 Utilities in support of old_snapshot_threshold pageinspect 1.9 Inspect the contents of database pages at a low level pg_buffercache 1.3 Examine the shared buffer cache pg_cron 1.6 Job scheduler for PostgreSQL pg_freespacemap 1.2 Examine the free space map (FSM) pg_permissions 1.3 View object permissions and compare them with the desired state pg_prewarm 1.2 Prewarm relation data pg_proctab Placeholder - see pg_proctab\u0026ndash;0.0.10-compat.control pg_stat_statements 1.9 Track planning and execution statistics of all SQL statements executed pg_surgery 1.0 Extension to perform surgery on a damaged relation pg_trgm 1.6 Text similarity measurement and index searching based on trigrams pg_visibility 1.2 Examine the visibility map (VM) and page-level visibility info pgaudit 1.6.3 Provides auditing functionality pgauditlogtofile 1.6 pgAudit addon to redirect audit entries to an independent file pgcrypto 1.3 Cryptographic functions pgnodemx 1.7 SQL functions that allow capture of node OS metrics from PostgreSQL pgrowlocks 1.2 Show row-level locking information pgstattuple 1.5 Show tuple-level statistics plpgsql 1.0 PL/pgSQL procedural language plpython3u 1.0 PL/Python3U untrusted procedural language pltcl 1.0 PL/Tcl procedural language pltclu 1.0 PL/TclU untrusted procedural language postgres_fdw 1.1 Foreign-data wrapper for remote PostgreSQL servers refint 1.0 Functions for implementing referential integrity (obsolete) seg 1.4 Data type for representing line segments or floating-point intervals set_user 4.1.0 Similar to SET ROLE but with added logging sslinfo 1.2 Information about SSL certificates tablefunc 1.0 Functions that manipulate whole tables, including crosstab tcn 1.0 Triggered change notifications timescaledb 2.18.2 Enables scalable inserts and complex queries for time-series data (Apache 2 Edition) tsm_system_rows 1.0 TABLESAMPLE method which accepts number of rows as a limit tsm_system_time 1.0 TABLESAMPLE method which accepts time in milliseconds as a limit unaccent 1.1 Text search dictionary that removes accents uuid-ossp 1.1 Generate universally unique identifiers (UUIDs) xml2 1.1 XPath querying and XSLT The following extensions are additionally included in the Postgis images. Name Default Version Comment address_standardizer 3.4.4 Used to parse an address into constituent elements. Generally used to support geocoding address normalization step. address_standardizer_data_us 3.4.4 Address Standardizer US dataset example postgis 3.4.4 PostGIS geometry and geography spatial types and functions postgis_raster 3.4.4 PostGIS raster types and functions postgis_sfcgal 3.4.4 PostGIS SFCGAL functions postgis_tiger_geocoder 3.4.4 PostGIS tiger geocoder and reverse geocoder postgis_topology 3.4.4 PostGIS topology spatial types and functions "},{"id":37,"href":"/CYBERTEC-pg-operator/extensions/pg15/","title":"PostgreSQL 15","parent":"Extensions","content":" The extensions listed are included in the standard images. This list refers to PostgreSQL 15. Name Default Version Comment adminpack 2.1 Administrative functions for PostgreSQL amcheck 1.3 Functions for verifying relation integrity autoinc 1.0 Functions for autoincrementing fields bloom 1.0 Bloom access method - signature file based index btree_gin 1.3 Support for indexing common datatypes in GIN btree_gist 1.7 Support for indexing common datatypes in GiST citext 1.6 Data type for case-insensitive character strings credcheck 3.0.0 credcheck - PostgreSQL plain text credential checker cube 1.5 Data type for multidimensional cubes dblink 1.2 Connect to other PostgreSQL databases from within a database dict_int 1.0 Text search dictionary template for integers dict_xsyn 1.0 Text search dictionary template for extended synonym processing earthdistance 1.1 Calculate great-circle distances on the surface of the Earth file_fdw 1.0 Foreign-data wrapper for flat file access fuzzystrmatch 1.1 Determine similarities and distance between strings hstore 1.8 Data type for storing sets of (key, value) pairs hstore_plperl 1.0 Transform between hstore and plperl hstore_plperlu 1.0 Transform between hstore and plperlu hstore_plpython3u 1.0 Transform between hstore and plpython3u insert_username 1.0 Functions for tracking who changed a table intagg 1.1 Integer aggregator and enumerator (obsolete) intarray 1.5 Functions, operators, and index support for 1-D arrays of integers isn 1.2 Data types for international product numbering standards jsonb_plperl 1.0 Transform between jsonb and plperl jsonb_plperlu 1.0 Transform between jsonb and plperlu jsonb_plpython3u 1.0 Transform between jsonb and plpython3u lo 1.1 Large Object maintenance ltree 1.2 Data type for hierarchical tree-like structures ltree_plpython3u 1.0 Transform between ltree and plpython3u moddatetime 1.0 Functions for tracking last modification time old_snapshot 1.0 Utilities in support of old_snapshot_threshold pageinspect 1.11 Inspect the contents of database pages at a low level pg_buffercache 1.3 Examine the shared buffer cache pg_cron 1.6 Job scheduler for PostgreSQL pg_freespacemap 1.2 Examine the free space map (FSM) pg_permissions 1.3 View object permissions and compare them with the desired state pg_prewarm 1.2 Prewarm relation data pg_proctab Placeholder - see pg_proctab\u0026ndash;0.0.10-compat.control pg_stat_statements 1.10 Track planning and execution statistics of all SQL statements executed pg_surgery 1.0 Extension to perform surgery on a damaged relation pg_trgm 1.6 Text similarity measurement and index searching based on trigrams pg_visibility 1.2 Examine the visibility map (VM) and page-level visibility info pg_walinspect 1.0 Functions to inspect contents of PostgreSQL Write-Ahead Log pgaudit 1.7 Provides auditing functionality pgauditlogtofile 1.6 pgAudit addon to redirect audit entries to an independent file pgcrypto 1.3 Cryptographic functions pgnodemx 1.7 SQL functions that allow capture of node OS metrics from PostgreSQL pgrowlocks 1.2 Show row-level locking information pgstattuple 1.5 Show tuple-level statistics plpgsql 1.0 PL/pgSQL procedural language plpython3u 1.0 PL/Python3U untrusted procedural language pltcl 1.0 PL/Tcl procedural language pltclu 1.0 PL/TclU untrusted procedural language postgres_fdw 1.1 Foreign-data wrapper for remote PostgreSQL servers refint 1.0 Functions for implementing referential integrity (obsolete) seg 1.4 Data type for representing line segments or floating-point intervals set_user 4.1.0 Similar to SET ROLE but with added logging sslinfo 1.2 Information about SSL certificates tablefunc 1.0 Functions that manipulate whole tables, including crosstab tcn 1.0 Triggered change notifications timescaledb 2.18.2 Enables scalable inserts and complex queries for time-series data (Apache 2 Edition) tsm_system_rows 1.0 TABLESAMPLE method which accepts number of rows as a limit tsm_system_time 1.0 TABLESAMPLE method which accepts time in milliseconds as a limit unaccent 1.1 Text search dictionary that removes accents uuid-ossp 1.1 Generate universally unique identifiers (UUIDs) xml2 1.1 XPath querying and XSLT The following extensions are additionally included in the Postgis images. Name Default Version Comment address_standardizer 3.4.4 Used to parse an address into constituent elements. Generally used to support geocoding address normalization step. address_standardizer_data_us 3.4.4 Address Standardizer US dataset example postgis 3.4.4 PostGIS geometry and geography spatial types and functions postgis_raster 3.4.4 PostGIS raster types and functions postgis_sfcgal 3.4.4 PostGIS SFCGAL functions postgis_tiger_geocoder 3.4.4 PostGIS tiger geocoder and reverse geocoder postgis_topology 3.4.4 PostGIS topology spatial types and functions ","description":" The extensions listed are included in the standard images. This list refers to PostgreSQL 15. Name Default Version Comment adminpack 2.1 Administrative functions for PostgreSQL amcheck 1.3 Functions for verifying relation integrity autoinc 1.0 Functions for autoincrementing fields bloom 1.0 Bloom access method - signature file based index btree_gin 1.3 Support for indexing common datatypes in GIN btree_gist 1.7 Support for indexing common datatypes in GiST citext 1.6 Data type for case-insensitive character strings credcheck 3.0.0 credcheck - PostgreSQL plain text credential checker cube 1.5 Data type for multidimensional cubes dblink 1.2 Connect to other PostgreSQL databases from within a database dict_int 1.0 Text search dictionary template for integers dict_xsyn 1.0 Text search dictionary template for extended synonym processing earthdistance 1.1 Calculate great-circle distances on the surface of the Earth file_fdw 1.0 Foreign-data wrapper for flat file access fuzzystrmatch 1.1 Determine similarities and distance between strings hstore 1.8 Data type for storing sets of (key, value) pairs hstore_plperl 1.0 Transform between hstore and plperl hstore_plperlu 1.0 Transform between hstore and plperlu hstore_plpython3u 1.0 Transform between hstore and plpython3u insert_username 1.0 Functions for tracking who changed a table intagg 1.1 Integer aggregator and enumerator (obsolete) intarray 1.5 Functions, operators, and index support for 1-D arrays of integers isn 1.2 Data types for international product numbering standards jsonb_plperl 1.0 Transform between jsonb and plperl jsonb_plperlu 1.0 Transform between jsonb and plperlu jsonb_plpython3u 1.0 Transform between jsonb and plpython3u lo 1.1 Large Object maintenance ltree 1.2 Data type for hierarchical tree-like structures ltree_plpython3u 1.0 Transform between ltree and plpython3u moddatetime 1.0 Functions for tracking last modification time old_snapshot 1.0 Utilities in support of old_snapshot_threshold pageinspect 1.11 Inspect the contents of database pages at a low level pg_buffercache 1.3 Examine the shared buffer cache pg_cron 1.6 Job scheduler for PostgreSQL pg_freespacemap 1.2 Examine the free space map (FSM) pg_permissions 1.3 View object permissions and compare them with the desired state pg_prewarm 1.2 Prewarm relation data pg_proctab Placeholder - see pg_proctab\u0026ndash;0.0.10-compat.control pg_stat_statements 1.10 Track planning and execution statistics of all SQL statements executed pg_surgery 1.0 Extension to perform surgery on a damaged relation pg_trgm 1.6 Text similarity measurement and index searching based on trigrams pg_visibility 1.2 Examine the visibility map (VM) and page-level visibility info pg_walinspect 1.0 Functions to inspect contents of PostgreSQL Write-Ahead Log pgaudit 1.7 Provides auditing functionality pgauditlogtofile 1.6 pgAudit addon to redirect audit entries to an independent file pgcrypto 1.3 Cryptographic functions pgnodemx 1.7 SQL functions that allow capture of node OS metrics from PostgreSQL pgrowlocks 1.2 Show row-level locking information pgstattuple 1.5 Show tuple-level statistics plpgsql 1.0 PL/pgSQL procedural language plpython3u 1.0 PL/Python3U untrusted procedural language pltcl 1.0 PL/Tcl procedural language pltclu 1.0 PL/TclU untrusted procedural language postgres_fdw 1.1 Foreign-data wrapper for remote PostgreSQL servers refint 1.0 Functions for implementing referential integrity (obsolete) seg 1.4 Data type for representing line segments or floating-point intervals set_user 4.1.0 Similar to SET ROLE but with added logging sslinfo 1.2 Information about SSL certificates tablefunc 1.0 Functions that manipulate whole tables, including crosstab tcn 1.0 Triggered change notifications timescaledb 2.18.2 Enables scalable inserts and complex queries for time-series data (Apache 2 Edition) tsm_system_rows 1.0 TABLESAMPLE method which accepts number of rows as a limit tsm_system_time 1.0 TABLESAMPLE method which accepts time in milliseconds as a limit unaccent 1.1 Text search dictionary that removes accents uuid-ossp 1.1 Generate universally unique identifiers (UUIDs) xml2 1.1 XPath querying and XSLT The following extensions are additionally included in the Postgis images. Name Default Version Comment address_standardizer 3.4.4 Used to parse an address into constituent elements. Generally used to support geocoding address normalization step. address_standardizer_data_us 3.4.4 Address Standardizer US dataset example postgis 3.4.4 PostGIS geometry and geography spatial types and functions postgis_raster 3.4.4 PostGIS raster types and functions postgis_sfcgal 3.4.4 PostGIS SFCGAL functions postgis_tiger_geocoder 3.4.4 PostGIS tiger geocoder and reverse geocoder postgis_topology 3.4.4 PostGIS topology spatial types and functions "},{"id":38,"href":"/CYBERTEC-pg-operator/extensions/pg16/","title":"PostgreSQL 16","parent":"Extensions","content":" The extensions listed are included in the standard images. This list refers to PostgreSQL 16. Name Default Version Comment adminpack 2.1 Administrative functions for PostgreSQL amcheck 1.3 Functions for verifying relation integrity autoinc 1.0 Functions for autoincrementing fields bloom 1.0 Bloom access method - signature file based index btree_gin 1.3 Support for indexing common datatypes in GIN btree_gist 1.7 Support for indexing common datatypes in GiST citext 1.6 Data type for case-insensitive character strings credcheck 3.0.0 credcheck - PostgreSQL plain text credential checker cube 1.5 Data type for multidimensional cubes dblink 1.2 Connect to other PostgreSQL databases from within a database dict_int 1.0 Text search dictionary template for integers dict_xsyn 1.0 Text search dictionary template for extended synonym processing earthdistance 1.2 Calculate great-circle distances on the surface of the Earth file_fdw 1.0 Foreign-data wrapper for flat file access fuzzystrmatch 1.2 Determine similarities and distance between strings hstore 1.8 Data type for storing sets of (key, value) pairs hstore_plperl 1.0 Transform between hstore and plperl hstore_plperlu 1.0 Transform between hstore and plperlu hstore_plpython3u 1.0 Transform between hstore and plpython3u insert_username 1.0 Functions for tracking who changed a table intagg 1.1 Integer aggregator and enumerator (obsolete) intarray 1.5 Functions, operators, and index support for 1-D arrays of integers isn 1.2 Data types for international product numbering standards jsonb_plperl 1.0 Transform between jsonb and plperl jsonb_plperlu 1.0 Transform between jsonb and plperlu jsonb_plpython3u 1.0 Transform between jsonb and plpython3u lo 1.1 Large Object maintenance ltree 1.2 Data type for hierarchical tree-like structures ltree_plpython3u 1.0 Transform between ltree and plpython3u moddatetime 1.0 Functions for tracking last modification time old_snapshot 1.0 Utilities in support of old_snapshot_threshold pageinspect 1.12 Inspect the contents of database pages at a low level pg_buffercache 1.4 Examine the shared buffer cache pg_cron 1.6 Job scheduler for PostgreSQL pg_freespacemap 1.2 Examine the free space map (FSM) pg_permissions 1.3 View object permissions and compare them with the desired state pg_prewarm 1.2 Prewarm relation data pg_proctab Placeholder - see pg_proctab\u0026ndash;0.0.10-compat.control pg_stat_statements 1.10 Track planning and execution statistics of all SQL statements executed pg_surgery 1.0 Extension to perform surgery on a damaged relation pg_trgm 1.6 Text similarity measurement and index searching based on trigrams pg_visibility 1.2 Examine the visibility map (VM) and page-level visibility info pg_walinspect 1.1 Functions to inspect contents of PostgreSQL Write-Ahead Log pgaudit 16.1 Provides auditing functionality pgauditlogtofile 1.6 pgAudit addon to redirect audit entries to an independent file pgcrypto 1.3 Cryptographic functions pgnodemx 1.7 SQL functions that allow capture of node OS metrics from PostgreSQL pgrowlocks 1.2 Show row-level locking information pgstattuple 1.5 Show tuple-level statistics plpgsql 1.0 PL/pgSQL procedural language plpython3u 1.0 PL/Python3U untrusted procedural language pltcl 1.0 PL/Tcl procedural language pltclu 1.0 PL/TclU untrusted procedural language postgres_fdw 1.1 Foreign-data wrapper for remote PostgreSQL servers refint 1.0 Functions for implementing referential integrity (obsolete) seg 1.4 Data type for representing line segments or floating-point intervals set_user 4.1.0 Similar to SET ROLE but with added logging sslinfo 1.2 Information about SSL certificates tablefunc 1.0 Functions that manipulate whole tables, including crosstab tcn 1.0 Triggered change notifications timescaledb 2.18.2 Enables scalable inserts and complex queries for time-series data (Apache 2 Edition) tsm_system_rows 1.0 TABLESAMPLE method which accepts number of rows as a limit tsm_system_time 1.0 TABLESAMPLE method which accepts time in milliseconds as a limit unaccent 1.1 Text search dictionary that removes accents uuid-ossp 1.1 Generate universally unique identifiers (UUIDs) xml2 1.1 XPath querying and XSLT The following extensions are additionally included in the Postgis images. Name Default Version Comment address_standardizer 3.4.4 Used to parse an address into constituent elements. Generally used to support geocoding address normalization step. address_standardizer_data_us 3.4.4 Address Standardizer US dataset example postgis 3.4.4 PostGIS geometry and geography spatial types and functions postgis_raster 3.4.4 PostGIS raster types and functions postgis_sfcgal 3.4.4 PostGIS SFCGAL functions postgis_tiger_geocoder 3.4.4 PostGIS tiger geocoder and reverse geocoder postgis_topology 3.4.4 PostGIS topology spatial types and functions ","description":" The extensions listed are included in the standard images. This list refers to PostgreSQL 16. Name Default Version Comment adminpack 2.1 Administrative functions for PostgreSQL amcheck 1.3 Functions for verifying relation integrity autoinc 1.0 Functions for autoincrementing fields bloom 1.0 Bloom access method - signature file based index btree_gin 1.3 Support for indexing common datatypes in GIN btree_gist 1.7 Support for indexing common datatypes in GiST citext 1.6 Data type for case-insensitive character strings credcheck 3.0.0 credcheck - PostgreSQL plain text credential checker cube 1.5 Data type for multidimensional cubes dblink 1.2 Connect to other PostgreSQL databases from within a database dict_int 1.0 Text search dictionary template for integers dict_xsyn 1.0 Text search dictionary template for extended synonym processing earthdistance 1.2 Calculate great-circle distances on the surface of the Earth file_fdw 1.0 Foreign-data wrapper for flat file access fuzzystrmatch 1.2 Determine similarities and distance between strings hstore 1.8 Data type for storing sets of (key, value) pairs hstore_plperl 1.0 Transform between hstore and plperl hstore_plperlu 1.0 Transform between hstore and plperlu hstore_plpython3u 1.0 Transform between hstore and plpython3u insert_username 1.0 Functions for tracking who changed a table intagg 1.1 Integer aggregator and enumerator (obsolete) intarray 1.5 Functions, operators, and index support for 1-D arrays of integers isn 1.2 Data types for international product numbering standards jsonb_plperl 1.0 Transform between jsonb and plperl jsonb_plperlu 1.0 Transform between jsonb and plperlu jsonb_plpython3u 1.0 Transform between jsonb and plpython3u lo 1.1 Large Object maintenance ltree 1.2 Data type for hierarchical tree-like structures ltree_plpython3u 1.0 Transform between ltree and plpython3u moddatetime 1.0 Functions for tracking last modification time old_snapshot 1.0 Utilities in support of old_snapshot_threshold pageinspect 1.12 Inspect the contents of database pages at a low level pg_buffercache 1.4 Examine the shared buffer cache pg_cron 1.6 Job scheduler for PostgreSQL pg_freespacemap 1.2 Examine the free space map (FSM) pg_permissions 1.3 View object permissions and compare them with the desired state pg_prewarm 1.2 Prewarm relation data pg_proctab Placeholder - see pg_proctab\u0026ndash;0.0.10-compat.control pg_stat_statements 1.10 Track planning and execution statistics of all SQL statements executed pg_surgery 1.0 Extension to perform surgery on a damaged relation pg_trgm 1.6 Text similarity measurement and index searching based on trigrams pg_visibility 1.2 Examine the visibility map (VM) and page-level visibility info pg_walinspect 1.1 Functions to inspect contents of PostgreSQL Write-Ahead Log pgaudit 16.1 Provides auditing functionality pgauditlogtofile 1.6 pgAudit addon to redirect audit entries to an independent file pgcrypto 1.3 Cryptographic functions pgnodemx 1.7 SQL functions that allow capture of node OS metrics from PostgreSQL pgrowlocks 1.2 Show row-level locking information pgstattuple 1.5 Show tuple-level statistics plpgsql 1.0 PL/pgSQL procedural language plpython3u 1.0 PL/Python3U untrusted procedural language pltcl 1.0 PL/Tcl procedural language pltclu 1.0 PL/TclU untrusted procedural language postgres_fdw 1.1 Foreign-data wrapper for remote PostgreSQL servers refint 1.0 Functions for implementing referential integrity (obsolete) seg 1.4 Data type for representing line segments or floating-point intervals set_user 4.1.0 Similar to SET ROLE but with added logging sslinfo 1.2 Information about SSL certificates tablefunc 1.0 Functions that manipulate whole tables, including crosstab tcn 1.0 Triggered change notifications timescaledb 2.18.2 Enables scalable inserts and complex queries for time-series data (Apache 2 Edition) tsm_system_rows 1.0 TABLESAMPLE method which accepts number of rows as a limit tsm_system_time 1.0 TABLESAMPLE method which accepts time in milliseconds as a limit unaccent 1.1 Text search dictionary that removes accents uuid-ossp 1.1 Generate universally unique identifiers (UUIDs) xml2 1.1 XPath querying and XSLT The following extensions are additionally included in the Postgis images. Name Default Version Comment address_standardizer 3.4.4 Used to parse an address into constituent elements. Generally used to support geocoding address normalization step. address_standardizer_data_us 3.4.4 Address Standardizer US dataset example postgis 3.4.4 PostGIS geometry and geography spatial types and functions postgis_raster 3.4.4 PostGIS raster types and functions postgis_sfcgal 3.4.4 PostGIS SFCGAL functions postgis_tiger_geocoder 3.4.4 PostGIS tiger geocoder and reverse geocoder postgis_topology 3.4.4 PostGIS topology spatial types and functions "},{"id":39,"href":"/CYBERTEC-pg-operator/extensions/","title":"Extensions","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":40,"href":"/CYBERTEC-pg-operator/monitoring/","title":"Monitoring","parent":"CPO (CYBERTEC-PG-Operator)","content":"The CPO-Project has prepared severall Tools which allows to setup a Monitoring-Stack including Alerting and Metric-Viewer. These Stack is based on:\nPrometheus Alertmanager Grafana exporter-container CPO has prepared an own Exporter for the PostgreSQl-Pod which can used as a sidecar.\nSetting up the Monitoring Stack To setup the Monitoring-Stack we suggest that you create an own namespace and use the prepared kustomization file inside the Operator-Tutorials.\n$ kubectl create namespace cpo-monitoring namespace/cpo-monitoring created $ kubectl get pods -n cpo-monitoring No resources found in cpo-monitoring namespace. git clone https://github.com/cybertec-postgresql/CYBERTEC-operator-tutorial cd CYBERTEC-operator-tutorial/setup/monitoring # Hint: Please check if youn want to use a specific storage-class the file pvcs.yaml and add your storageclass on the commented part. Please ensure that you removed the comment-char. $ kubectl apply -n cpo-monitoring -k . serviceaccount/cpo-monitoring created serviceaccount/cpo-monitoring-tools created clusterrole.rbac.authorization.k8s.io/cpo-monitoring unchanged clusterrolebinding.rbac.authorization.k8s.io/cpo-monitoring unchanged configmap/alertmanager-config created configmap/alertmanager-rules-config created configmap/cpo-prometheus-cm created configmap/grafana-dashboards created configmap/grafana-datasources created secret/grafana-secret created service/cpo-monitoring-alertmanager created service/cpo-monitoring-grafana created service/cpo-monitoring-prometheus created persistentvolumeclaim/alertmanager-pvc created persistentvolumeclaim/grafana-pvc created persistentvolumeclaim/prometheus-pvc created deployment.apps/cpo-monitoring-alertmanager created deployment.apps/cpo-monitoring-grafana created deployment.apps/cpo-monitoring-prometheus created Hint: If you\u0026#39;re not running Openshift you will get a error like this: error: resource mapping not found for name: \u0026#34;grafana\u0026#34; namespace: \u0026#34;\u0026#34; from \u0026#34;.\u0026#34;: no matches for kind \u0026#34;Route\u0026#34; in version \u0026#34;route.openshift.io/v1\u0026#34; ensure CRDs are installed first You can ignore this, because it depends on an object with the type route which is part of Openshift. It is not needed replaced by ingress-rules or an loadbalancer-service. After installing the Monitoring-Stack we\u0026rsquo;re able to check the created pods inside the namespace\n$ kubectl get pods -n cpo-monitoring ---------------------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cpo-monitoring-alertmanager-5bb8bc79f7-8pdv4 | 1/1 | Running | 0 | 3m35s cpo-monitoring-grafana-7c7c4f787b-jbj2f | 1/1 | Running | 0 | 3m35s cpo-monitoring-prometheus-67969b757f-k26jd | 1/1 | Running | 0 | 3m35s The configuration of this monitoring-stack is based on severall configmaps which can be modified.\nPrometheus-Configuration Alertmanager-Configuration Grafana-Configuration Configure a PostgreSQL-Cluster to allow Prometheus to gather metrics To allow Prometheus to gather metrics from your cluster you need to do some small modfications on the Cluster-Manifest. We need to create the monitor-object for this:\nkubectl edit postgresqls.cpo.opensource.cybertec.at cluster-1 ... spec: ... monitor: image: docker.io/cybertecpostgresql/cybertec-pg-container:exporter-16.2-1 The Operator will add automatically the monitoring sidecar to your pods, create a new postgres-user and add some structure inside the postgres-database to enable everthing needed for the Monitoring. Also every Ressource of your Cluster will get a new label: cpo_monitoring_stack=true. This is needed for Prometheus to identify all clusters which should be added to the monitoring. Removing this label will stop Prometheus to gather data from this cluster.\nAfter changing your Cluster-Manifest the Pods needs to be recreated which is done by a rolling update. After this you can see that the pod has now more than just one container.\nkubectl get pods ----------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cluster-1-0 | 2/2 | Running | 0 | 54s cluster-1-1 | 2/2 | Running | 0 | 31s You can check the logs to see that the exporter is working and with curl you can see the output of the exporter.\nkubectl logs cluster-1-0 -c postgres-exporter kubectl exec --stdin --tty cluster-1-0 -c postgres-exporter -- /bin/bash [exporter@cluster-1-0 /]# curl http://127.0.0.1:9187/metrics You can now setup a LoadBalancer-Service or create an Ingress-Rule to allow access von outside to the grafana. Alternativ you can use a port-forward.\nLoadBalancer or Nodeport Ingress-Rule Port-Forwarding $ kubectl get pods -n cpo-monitoring ---------------------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cpo-monitoring-alertmanager-5bb8bc79f7-8pdv4 | 1/1 | Running | 0 | 6m42s cpo-monitoring-grafana-7c7c4f787b-jbj2f | 1/1 | Running | 0 | 6m42s cpo-monitoring-prometheus-67969b757f-k26jd | 1/1 | Running | 0 | 6m42s $ kubectl port-forward cpo-monitoring-grafana-7c7c4f787b-jbj2f -n cpo-monitoring 9000:9000 Forwarding from 127.0.0.1:9000 -\u0026gt; 9000 Forwarding from [::1]:9000 -\u0026gt; 9000 Call http://localhost:9000 in the Browser\nUse a Route (Openshift only) kubectl get route -n cpo-monitoring Use the Route-Adress to access Grafana\n","description":"The CPO-Project has prepared severall Tools which allows to setup a Monitoring-Stack including Alerting and Metric-Viewer. These Stack is based on:\nPrometheus Alertmanager Grafana exporter-container CPO has prepared an own Exporter for the PostgreSQl-Pod which can used as a sidecar.\nSetting up the Monitoring Stack To setup the Monitoring-Stack we suggest that you create an own namespace and use the prepared kustomization file inside the Operator-Tutorials.\n"},{"id":41,"href":"/CYBERTEC-pg-operator/cluster_upgrade/","title":"Major-Upgrade","parent":"CPO (CYBERTEC-PG-Operator)","content":"CPO enables the use of the in-place upgrade, which makes it possible to upgrade a cluster to a new PG major. For this purpose, pg_upgrade is used in the background.\nPAY ATTENTION: Note that an in-place upgrade generates both a pod restore in the form of a rolling update and an operational interruption of the cluster during the actual execution of the restore.\nHow does the upgrade work? Preconditions: Pod restart - Use the rolling update strategy to replace all pods based on the new ENV PGVERSION with the version you want to update to. Check - Check that the new PGVERSION is larger than the previously used one. Check whether the new PGVERSION is larger than the previously used one and the maintenance mode of the cluster must be deactivated. In addition, the replicas should not have a high lag. Preliminary checks use initdb to prepare a new data_dir (data_new) based on the new PGVERSION. check the upgrade possibility with pg_upgrade --check HINT: If one of the steps is aborted, a cleanup is performed\nPrepare the Upgrade remove dependencies that can cause problems. For example, the extensions pg_stat_statements and pgaudit. activate the maintenance mode of the cluster terminate PostgreSQL in an orderly manner check pg_controldata for the checkpoint position and wait until all replicas apply the latest checkpoint location use port 5432 for rsyncd and start it Start the Upgrade Call pg_upgrade -k to start the Upgrade ATTENTION if the process failed, we need to rollback, if it was sucessful we\u0026rsquo;re reaching the point of no return\nRename the directories. data -\u0026gt; data_old and data_new -\u0026gt; data Update the Patroni.config (postgres.yml) Call Checkpoint on every replica and trigger rsync on the Replicas Wait for Replicas to complete rsxnc. Timeout: 300 Stop rsyncd on Primary and remove ininitialize key from DCS, because its based on the old sysid Start Patroni on the Primary and start the postgres locally Reset custom staticstics, warmup the Memory and start Analyze in stages in separate threads Wait for every Replica to become ready Disable the maintenance mode for the Cluster Restore custom statistics, analyze these tables and restore dropped objetcs from Prepare the upgrade Completion of the upgrade Drop directory data_old Trigger new Backup How a rollback is working? Stop rsynd if its running Disable the maintenance mode for the Cluster Drop directory data_new How to trigger a In-Place-Upgrade with cpo? spec: postgresql: version: \u0026#34;16\u0026#34; To trigger an In-Place-Upgrade you have just to increase the parameter spec.postgresql.version. If you choose a valid number the Operator will start with the prozedure, described above. If you choosse a not allowed value, you will give an error and if you decrease the value, the operator will just ignore it with the following log-Entry.\nOperator-Log ","description":"CPO enables the use of the in-place upgrade, which makes it possible to upgrade a cluster to a new PG major. For this purpose, pg_upgrade is used in the background.\nPAY ATTENTION: Note that an in-place upgrade generates both a pod restore in the form of a rolling update and an operational interruption of the cluster during the actual execution of the restore.\nHow does the upgrade work? Preconditions: Pod restart - Use the rolling update strategy to replace all pods based on the new ENV PGVERSION with the version you want to update to. Check - Check that the new PGVERSION is larger than the previously used one. Check whether the new PGVERSION is larger than the previously used one and the maintenance mode of the cluster must be deactivated. In addition, the replicas should not have a high lag. Preliminary checks use initdb to prepare a new data_dir (data_new) based on the new PGVERSION. check the upgrade possibility with pg_upgrade --check HINT: If one of the steps is aborted, a cleanup is performed\n"},{"id":42,"href":"/CYBERTEC-pg-operator/multisite/","title":"Multisite","parent":"CPO (CYBERTEC-PG-Operator)","content":"Multisite is a function specially developed for Patroni that makes it possible to combine two separate Patroni clusters into a common cluster unit. Separate in this context means that the clusters run independently of each other and can even be located on different Kubernetes clusters. With Multisite, both clusters benefit from the well-known Patroni features such as automatic failover and demotion of members, resulting in a significant extension compared to a conventional standby cluster. This feature significantly improves high availability and redundancy by managing multiple geographically or infrastructurally separated clusters as one logical unit. This allows one cluster to seamlessly transition to another in the event of a failure without having to rely on manual switchovers or third-party replication solutions.\nPrerequisites In order to set up the multisite PostgreSQL operator you will need the following:\nTwo or more Kubernetes or OpenShift clusters (also possible with bare metal or VMs) Kubernetes version 1.25+, OpenShift version 4.12+. Support for defining LoadBalancer services with external IP addresses that are accessible from the other cluster(s). Persistent volumes with must be available (only ReadWriteOnce capability is needed). A separate VM or Kubernetes/OpenShift cluster to provide quorum (if using less then three Kubernetes or OpenShift clusters). For high availability there should not be a shared point of failure between the quorum and the two Kubernetes clusters. VM or a LoadBalancer IP must be accessible on ports 2379/2380 to the two other clusters. 2 vCPU and 2 GB of memory and 20GB of persistent storage is needed for the quorum site. Set up etcd cluster with 3 sites accessible from each of the sites. etcd needs to support API version 3. For backups an object storage system with S3 compatible API is needed. Minio, Ceph and major cloud provider object storages are known to work. An additional etcd is set up for Multisite, which spans the Kubernetes or Openshift clusters and must contain the quorum. Architecture Helm based deployment of the multisite operator contains two helm charts, postgres-operator and postgres-cluster. The first is used to deploy the operator and associated objects to a single Kubernetes cluster. The operator is responsible for managing PostgreSQL clusters based on Custom Resource Definitions (CRDs) of type postgresqls/pg.\nThe diagram contains in green the Helm charts that are used to deploy operator and clusters, in blue the objects deployed by the operator helm chart and in gold the objects deployed by the cluster chart.\nOperator helm chart deployed objects have the following purposes:\ndeployments/postgres-operator - Deployment for the operator itself. opconfig/postgres-operator - Operator configuration parameters that are read on operator startup. These apply to all clusters managed by this operator. crd/operatorconfigurations.cpo.opensource.cybertec.at - Schema for the operator configuration. clusterrole/postgres-operator - Defines the Kubernetes API resource access used by the operator. Assigned to postgres-operator service account. clusterrole/postgres-pod - The Kubernetes API access needed by database pods. Access is needed to access leader status, config and other things. This is assigned to postgres-pod service account used by database pods. crd/postgresqls.cpo.opensource.cybertec.at - Schema for PostgreSQL cluster definitions. clusterrole/postgres-operator:users:{admin,edit,view} - If rbac.createAggregateClusterRoles is set then user facing roles are added for accessing the postgresqls CRDs. The cluster chart creates an instance of postgresqls CRD, which will be called cluster manifest from here on. When this cluster manifest is created operator will create the needed resources for the cluster. These include:\nstatefulset/$clustername - StatefulSet is responsible for creating and managing database pods and their associated PersistentVolumeClaims for storing the databases. Each database pod will run internally an instance of Patroni process, which will coordinate over the Kubernetes API initialization of the database, startup, leader election and other control plane actions.\nservice/$clustername,endpoints/$clustername - The main access point for users accessing the database. When load balancer is enabled in the CRD or multisite mode is enabled, this service will be set to be a LoadBalancer service and accessible from outside the Kubernetes cluster. The service is created without a selector. Instead, for leader elections database pods will update the IP address of this endpoint to point to the current leader.\nThe endpoint also holds annotations that determine the duration of the leader lease.\nIn multicluster operation mode the standby site leader will be in read-only mode.\nservice/$clustername-repl - Service that points to non-leader (read-only) instances of the database cluster.\nservice/$clustername-config - A headless service with an endpoint that holds Patroni configuration in annotations.\npoddisruptionbudget/postgres-$clustername-pdb - A pod disruption budget that does not allow Kubernetes to shut down pods in leader role. On some Kubernetes clusters kubernetes.enable_pod_disruption_budgets may need to be turned off to allow nodes to be drained for upgrades.\nCreate the CA The first step is to create a custom CA. An organisation name is required for this. You can also add further details about the country, district and location. The CA serves as the central authority that signs the certificates and thus guarantees the correctness of the certificate. In order to successfully complete the verification of a certificate, the CA\u0026rsquo;s certificate must be stored on the client system.\nORGANIZATION=MyCustomOrganization CA=$ORGANIZATION-RootCA mkdir $CA cd $CA # Creating the CA-Key openssl genpkey -algorithm EC -out $CA.key -pkeyopt ec_paramgen_curve:secp384r1 -pkeyopt ec_param_enc:named_curve -aes256 # Creating the CA-Certificate openssl req -x509 -new -nodes -key $CA.key -sha512 -days 1826 -out $CA.crt -subj \u0026#34;/CN=${ORGANIZATION} Root-CA/C=AT/ST=Lower Austria/L=Woellersdorf/O=${ORGANIZATION}\u0026#34; Create a custom Certificate The server needs a certificate signed by a CA and a private key so that it can claim to be trustworthy.\nIt is important that the CA certificate is stored as trustworthy with the client. Otherwise, no certificate check is possible. CN=cluster-1 DNS2=\u0026#34;${CN}-repl\u0026#34; DNS3=\u0026#34;${CN}-pooler\u0026#34; DNS4=\u0026#34;${CN}-pooler-repl\u0026#34; # Creating the private Key openssl genpkey -algorithm EC -out $CN.key -pkeyopt ec_paramgen_curve:secp384r1 -pkeyopt ec_param_enc:named_curve # Creating Certificate Signing Request (CSR)) openssl req -new -key $CN.key -out $CN.csr \\ -subj \u0026#34;/C=AT/ST=Lower Austria/L=Woellersdorf/O=${ORGANIZATION}/OU=OrgUnit/CN=${CN}\u0026#34; \\ -addext \u0026#34;subjectAltName=DNS:${CN},DNS:${DNS2},DNS:${DNS3},DNS:${DNS4}\u0026#34; # Sign CSR with the CA openssl x509 -req -in $CN.csr -CA $CA.crt -CAkey $CA.key -CAcreateserial -out $CN.crt -days 365 \\ -extfile \u0026lt;(echo -e \u0026#34;[ v3_req ]\\nsubjectAltName=DNS:${CN},DNS:${DNS2},DNS:${DNS3},DNS:${DNS4}\u0026#34;) -extensions v3_req Add Certicate to the Cluster For adding the Certificate to your cluster a secret on kubernetes is needed. There are two different options here. For the first option, a secret is created that contains all the necessary information. I.e.\nServer certificate Private server key CA certificate In the second variant, the CA certificate is separated and written in a separate secret. The advantage of this is that the CA only needs to be saved once and changed in the event of an update. First Option: Using one secret for all three objects kubectl create secret generic cluster-1-tls \\ --from-file=tls.crt=$CN.crt \\ --from-file=tls.key=$CN.key \\ --from-file=ca.crt=$CA.crt Finally, the definition is made in the cluster manifest so that the operator adapts the cluster.\napiVersion: \u0026#34;cpo.opensource.cybertec.at/v1\u0026#34; kind: postgresql ... metadata: name: cluster-1 spec: tls: secretName: \u0026#34;cluster-1-tls\u0026#34; caFile: \u0026#34;ca.crt\u0026#34; Second Option: Using a separat Secret for the CA kubectl create secret generic cpo-root-ca --from-file=ca.crt=ca.crt kubectl create secret generic cluster-1-tls \\ --from-file=tls.crt=$CN.crt \\ --from-file=tls.key=$CN.key \\ Finally, the definition is made in the cluster manifest so that the operator adapts the cluster.\napiVersion: \u0026#34;cpo.opensource.cybertec.at/v1\u0026#34; kind: postgresql metadata: name: cluster-1 spec: tls: secretName: \u0026#34;cluster-1-tls\u0026#34; caSecretName: \u0026#34;cpo-root-ca\u0026#34; caFile: \u0026#34;ca.crt\u0026#34; A regular check of the mounted certificates takes place automatically within the container. This check takes place every 5 minutes. If the certificates have been updated, the certificates are loaded automatically.\nIn addition to generating the certificates independently, cert-manager can also be used for this purpose. ","description":"Multisite is a function specially developed for Patroni that makes it possible to combine two separate Patroni clusters into a common cluster unit. Separate in this context means that the clusters run independently of each other and can even be located on different Kubernetes clusters. With Multisite, both clusters benefit from the well-known Patroni features such as automatic failover and demotion of members, resulting in a significant extension compared to a conventional standby cluster. This feature significantly improves high availability and redundancy by managing multiple geographically or infrastructurally separated clusters as one logical unit. This allows one cluster to seamlessly transition to another in the event of a failure without having to rely on manual switchovers or third-party replication solutions.\n"},{"id":43,"href":"/CYBERTEC-pg-operator/tutorials/","title":"Tutorials","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":44,"href":"/CYBERTEC-pg-operator/crd/","title":"References","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":45,"href":"/CYBERTEC-pg-operator/release_notes/","title":"Release-Notes","parent":"CPO (CYBERTEC-PG-Operator)","content":" 0.8.3 Fixes Majorupgrade updated for Patroni 4.x.x Supported Versions PG: 13 - 17 Patroni: 4.0.5 pgBackRest: 2.54.2 Kubernetes: 1.21 - 1.28 Openshift: 4.8 - 4.13 0.8.2 Features Added Clone-Functionality with pgBackRest Supported Versions PG: 13 - 17 Patroni: 3.3.2 pgBackRest: 2.54.0 Kubernetes: 1.21 - 1.28 Openshift: 4.8 - 4.13 0.8.1 Features Added pgbackrest to Monitoring Fixes Fixed role creation for monitoring Supported Versions PG: 13 - 17 Patroni: 3.3.2 pgBackRest: 2.53 Kubernetes: 1.21 - 1.28 Openshift: 4.8 - 4.13 0.8.0 Fixes Fixed role creation for monitoring. Fix for the use of gcs with pgBackRest Supported Versions PG: 13 - 16 \u0026amp; 17Beta2 Patroni: 3.3.2 pgBackRest: 2.53 Kubernetes: 1.21 - 1.28 Openshift: 4.8 - 4.13 0.7.1 Fixes Fixed role creation for monitoring. Fix for the use of gcs with pgBackRest Supported Versions PG: 13 - 16 \u0026amp; 17Beta2 Patroni: 3.3.2 pgBackRest: 2.53 Kubernetes: 1.21 - 1.28 Openshift: 4.8 - 4.13 0.7.0 Features Monitoring-Sidecar integrated via CRD Start with Monitoring Password-Hash per default set to scram-sha-256 pgBackRest with blockstorage using RepoHost Internal Certification-Management for RepoHost-Certificates Compatible with PG17Beta2 Changes API Change acid.zalan.do is replaced by cpo.opensource.cybertec.at - If you\u0026rsquo;re updating your Operator from previous Versions, please check this HowTo Migrate to new API Patroni-Compatibility has increased to Version 3.3.2 pgBackRest-Compatbility has increased to Version 2.52.1 Revision of the restore process Revision of the backup jobs Operator now using Rocky9 as Baseimage Updates Go-Package to 1.22.5 Fixes PDB Bug fixed - Single-Node Clusters are not creating PDBs anymore which can break Kubernetes-Update Wrong Templates inside Cronjobs fixed Supported Versions PG: 13 - 16 \u0026amp; 17Beta2 Patroni: 3.3.2 pgBackRest: 2.52.1 Kubernetes: 1.21 - 1.28 Openshift: 4.8 - 4.13 0.6.1 Release with fixes\nFixes Backup-Pod now runs with \u0026ldquo;best-effort\u0026rdquo; resource definition Der Init-Container fr die Wiederherstellung verwendet nun die gleiche Ressource-Definition wie der Datenbank-Container, wenn es keine spezifische Definition im Cluster-Manifest gibt (spec.backup.pgbackrest.resources) Software-Versions PostgreSQL: 15.3 14.8, 13.11, 12.15 Patroni: 3.0.4 pgBackRest: 2.47 OS: Rocky-Linux 9.1 (4.18) 0.6.0 Release with some improvements and stabilisation measuresm\nFeatures Added Pod Topology Spread Constraints Added support for TDE based on the CYBERTEC PostgreSQL Enterprise Images (Licensed Container Suite) Software-Versions PostgreSQL: 15.3 14.8, 13.11, 12.15 Patroni: 3.0.4 pgBackRest: 2.47 OS: Rocky-Linux 9.1 (4.18) 0.5.0 Release with new Software-Updates and some internal Improvements\nFeatures Updated to Zalando Operator 1.9 Fixes internal Problems with Cronjobs updates for some API-Definitions Software-Versions PostgreSQL: 15.2 14.7, 13.10, 12.14 Patroni: 3.0.2 pgBackRest: 2.45 OS: Rocky-Linux 9.1 (4.18) 0.3.0 Release with some improvements and stabilisation measuresm\nFixes missing pgbackrest_restore configmap fixed Software-Versions PostgreSQL: 15.1 14.7, 13.9, 12.13, 11.18 and 10.23 Patroni: 3.0.1 pgBackRest: 2.44 OS: Rocky-Linux 9.1 (4.18) 0.1.0 Initial Release as a Fork of the Zalando-Operator\nFeatures Added Support for pgBackRest (PoC-State) Stanza-create and Initial-Backup are executed automatically Schedule automatic updates (Full/Incremental/Differential-Backup) Securely store backups on AWS S3 and S3-compatible storage Software-Versions PostgreSQL: 14.6, 13.9, 12.13, 11.18 and 10.23 Patroni: 2.4.1 pgBackRest: 2.42 OS: Rocky-Linux 9.0 (4.18) ","description":" 0.8.3 Fixes Majorupgrade updated for Patroni 4.x.x Supported Versions PG: 13 - 17 Patroni: 4.0.5 pgBackRest: 2.54.2 Kubernetes: 1.21 - 1.28 Openshift: 4.8 - 4.13 0.8.2 Features Added Clone-Functionality with pgBackRest Supported Versions PG: 13 - 17 Patroni: 3.3.2 pgBackRest: 2.54.0 Kubernetes: 1.21 - 1.28 Openshift: 4.8 - 4.13 0.8.1 Features Added pgbackrest to Monitoring Fixes Fixed role creation for monitoring Supported Versions PG: 13 - 17 Patroni: 3.3.2 pgBackRest: 2.53 Kubernetes: 1.21 - 1.28 Openshift: 4.8 - 4.13 0.8.0 Fixes Fixed role creation for monitoring. Fix for the use of gcs with pgBackRest Supported Versions PG: 13 - 16 \u0026amp; 17Beta2 Patroni: 3.3.2 pgBackRest: 2.53 Kubernetes: 1.21 - 1.28 Openshift: 4.8 - 4.13 0.7.1 Fixes Fixed role creation for monitoring. Fix for the use of gcs with pgBackRest Supported Versions PG: 13 - 16 \u0026amp; 17Beta2 Patroni: 3.3.2 pgBackRest: 2.53 Kubernetes: 1.21 - 1.28 Openshift: 4.8 - 4.13 0.7.0 Features Monitoring-Sidecar integrated via CRD Start with Monitoring Password-Hash per default set to scram-sha-256 pgBackRest with blockstorage using RepoHost Internal Certification-Management for RepoHost-Certificates Compatible with PG17Beta2 Changes API Change acid.zalan.do is replaced by cpo.opensource.cybertec.at - If you\u0026rsquo;re updating your Operator from previous Versions, please check this HowTo Migrate to new API Patroni-Compatibility has increased to Version 3.3.2 pgBackRest-Compatbility has increased to Version 2.52.1 Revision of the restore process Revision of the backup jobs Operator now using Rocky9 as Baseimage Updates Go-Package to 1.22.5 Fixes PDB Bug fixed - Single-Node Clusters are not creating PDBs anymore which can break Kubernetes-Update Wrong Templates inside Cronjobs fixed Supported Versions PG: 13 - 16 \u0026amp; 17Beta2 Patroni: 3.3.2 pgBackRest: 2.52.1 Kubernetes: 1.21 - 1.28 Openshift: 4.8 - 4.13 0.6.1 Release with fixes\n"},{"id":46,"href":"/CYBERTEC-pg-operator/categories/","title":"Categories","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":47,"href":"/CYBERTEC-pg-operator/tags/","title":"Tags","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""}]